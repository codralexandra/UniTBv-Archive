{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8caedf",
   "metadata": {},
   "source": [
    "# Laborator 4: model de regresie liniara"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe82d8",
   "metadata": {},
   "source": [
    "## Descriere\n",
    "\n",
    "Descrierea problemei si a setului de date se gasesc in `data/Concrete_Readme.txt`. Setul de date este in fisierul `/data/Concrete_Data.xls`. Se va aplica model de regresie liniara pentru estimarea valorii de \"Concrete compressive strength\" (ultima coloana din setul de date), folosind ca intrari valorile din celelalte 8 coloane. \n",
    "\n",
    "Implementarea se va face folosind functii de NumPy. Se va folosi calcul vectorizat, pe cat posibil. \n",
    "\n",
    "**Termen de lucru: 3 săptămâni**\n",
    "\n",
    "Predarea temei se va face în perioada **12-15 noiembrie**, conform termenului limită alocat diferențiat pe grupe, de pe platforma elearning.\n",
    "\n",
    "\n",
    "Cerinte si precizari:\n",
    "1. Studentii se pot consulta intre ei, dar rezolvarile vor fi individuale. Necunoasterea codului prezentat inseamna nota 1 pentru tema curenta. \n",
    "2. Se se faca adnotari de tipuri pentru variabilele folosite, parametrii de intrare ai functiilor si tipurile de retur. Neindeplinirea acestei cerinte duce la injumatatirea notei."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8b8bb",
   "metadata": {},
   "source": [
    "## Importarea bibliotecilor, citirea datelor folosind Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "467fc940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:09:08.264583Z",
     "start_time": "2023-10-22T19:09:02.621527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in c:\\users\\elitebook\\anaconda3\\envs\\machine-learning\\lib\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pentru citire de fisier excel\n",
    "!pip install xlrd  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4973be47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:09:08.330869Z",
     "start_time": "2023-10-22T19:09:08.280500Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = './data/Concrete_Data.xls'\n",
    "data = pd.read_excel(data_path)\n",
    "assert data.shape == (1030, 9), f'Fisierul nu a fost citit corect: s-a citit un continut de forma {data.shape}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c719c99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:09:08.397042Z",
     "start_time": "2023-10-22T19:09:08.379980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement (component 1)(kg in a m^3 mixture)</th>\n",
       "      <th>Blast Furnace Slag (component 2)(kg in a m^3 mixture)</th>\n",
       "      <th>Fly Ash (component 3)(kg in a m^3 mixture)</th>\n",
       "      <th>Water  (component 4)(kg in a m^3 mixture)</th>\n",
       "      <th>Superplasticizer (component 5)(kg in a m^3 mixture)</th>\n",
       "      <th>Coarse Aggregate  (component 6)(kg in a m^3 mixture)</th>\n",
       "      <th>Fine Aggregate (component 7)(kg in a m^3 mixture)</th>\n",
       "      <th>Age (day)</th>\n",
       "      <th>Concrete compressive strength(MPa, megapascals)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.986111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.887366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.269535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.052780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.296075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement (component 1)(kg in a m^3 mixture)  \\\n",
       "0                                      540.0   \n",
       "1                                      540.0   \n",
       "2                                      332.5   \n",
       "3                                      332.5   \n",
       "4                                      198.6   \n",
       "\n",
       "   Blast Furnace Slag (component 2)(kg in a m^3 mixture)  \\\n",
       "0                                                0.0       \n",
       "1                                                0.0       \n",
       "2                                              142.5       \n",
       "3                                              142.5       \n",
       "4                                              132.4       \n",
       "\n",
       "   Fly Ash (component 3)(kg in a m^3 mixture)  \\\n",
       "0                                         0.0   \n",
       "1                                         0.0   \n",
       "2                                         0.0   \n",
       "3                                         0.0   \n",
       "4                                         0.0   \n",
       "\n",
       "   Water  (component 4)(kg in a m^3 mixture)  \\\n",
       "0                                      162.0   \n",
       "1                                      162.0   \n",
       "2                                      228.0   \n",
       "3                                      228.0   \n",
       "4                                      192.0   \n",
       "\n",
       "   Superplasticizer (component 5)(kg in a m^3 mixture)  \\\n",
       "0                                                2.5     \n",
       "1                                                2.5     \n",
       "2                                                0.0     \n",
       "3                                                0.0     \n",
       "4                                                0.0     \n",
       "\n",
       "   Coarse Aggregate  (component 6)(kg in a m^3 mixture)  \\\n",
       "0                                             1040.0      \n",
       "1                                             1055.0      \n",
       "2                                              932.0      \n",
       "3                                              932.0      \n",
       "4                                              978.4      \n",
       "\n",
       "   Fine Aggregate (component 7)(kg in a m^3 mixture)  Age (day)  \\\n",
       "0                                              676.0         28   \n",
       "1                                              676.0         28   \n",
       "2                                              594.0        270   \n",
       "3                                              594.0        365   \n",
       "4                                              825.5        360   \n",
       "\n",
       "   Concrete compressive strength(MPa, megapascals)   \n",
       "0                                         79.986111  \n",
       "1                                         61.887366  \n",
       "2                                         40.269535  \n",
       "3                                         41.052780  \n",
       "4                                         44.296075  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# afisarea primelor 5 linii din dataframe\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88712789",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:09:12.423594Z",
     "start_time": "2023-10-22T19:09:12.418819Z"
    }
   },
   "outputs": [],
   "source": [
    "# in matricea X vom avea valorile pentru trasaturile de intrare\n",
    "# in vectorul y vor fi valorile de iesire asociate\n",
    "# in ambele situatii: numpy arrays. In mod convenabil, y va fi vector coloana\n",
    "\n",
    "X:np.ndarray = data.iloc[:, 0:-1].values\n",
    "y:np.ndarray = data.iloc[:, -1].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5743e6bd",
   "metadata": {},
   "source": [
    "## Functii pentru date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fba7b3",
   "metadata": {},
   "source": [
    "Functia de mai jos este folosita pentru a face o permutare aleatoare a datelor initiale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32bf8f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:14:08.325770Z",
     "start_time": "2023-10-22T19:14:07.675963Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_shuffle(X:np.ndarray,y:np.ndarray)->tuple:\n",
    "    \"\"\"\n",
    "    This function shuffles the elements of two ndarrays randomly.\n",
    "    It returns a tuple of ndarrays.\n",
    "    \"\"\"\n",
    "    # obtinem indicii de linie, permutati\n",
    "    shuffled_indices:np.ndarray = np.random.permutation(np.arange(data.shape[0])) \n",
    "    #(data.shape = (1030,9)->data.shape[0]=1030)\n",
    "    #np.arrange(1030)->values are generated within the half-open interval [0, 1030) ->ndarray\n",
    "    #np.random.permutation(...) shuffles the indexes stored in np.arrange(...) -> ndarray\n",
    "    assert set(shuffled_indices) == set(np.arange(data.shape[0]))\n",
    "    # se folosesc indicii permutati\n",
    "    X_shuffled:np.ndarray = X[shuffled_indices]\n",
    "    y_shuffled:np.ndarray = y[shuffled_indices]\n",
    "    return X_shuffled, y_shuffled\n",
    "    \n",
    "X_shuffled, y_shuffled = random_shuffle(X, y)\n",
    "assert X_shuffled.shape == X.shape, f'Forme diferite: {X_shuffled.shape}, {X.shape}'\n",
    "assert y_shuffled.shape == y.shape, f'Forme diferite: {y_shuffled.shape}, {y.shape}'\n",
    "assert set(y_shuffled[:, 0]) == set(y[:, 0]), f'Valori diferite'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1343fb39",
   "metadata": {},
   "source": [
    "Avem nevoie de o functie care sa faca impartirea unei matrice (sau vector) in doua submatrice (respectiv 2 subvectori), in functie de procent `0 < p < 100` specificat. Functia va returna doua sumbatrice (subvectori), compuse din primele `p%` linii si restul de linii din matrice (vectorul) data. Functia va fi folosita pentru impartirea unui set de date in subset de antrenare si respectiv de testare.\n",
    "\n",
    "Exmeplu: daca avem matricea `A` de 200 de linii si 30 de coloane, iar `p=70`, atunci functia split_train_test de mai jos va returna un tuplu format din doua matrice. Prima matrice va avea 140 de linii si 40 de coloane (primele 140 de linii din `A`), a doua va avea restul de 60 de linii din `A`, tot 40 de coloane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b1e9b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T18:47:36.785819Z",
     "start_time": "2023-10-22T18:47:36.777457Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_train_test(vector_or_matrix:np.ndarray, p:int)->tuple:\n",
    "    \"\"\"\n",
    "    This function splits a vector or matrix by p percent.\n",
    "    It returns a tuple of 2 subarrays of the original vector or matrix.\n",
    "    \"\"\"\n",
    "    assert 0 < p < 100, f'Procentul p trebuie sa fie minim 1, maxim 99, am primit {p}'\n",
    "    rows:int = vector_or_matrix.shape[0]\n",
    "    rows_1:int = int((p*rows)/100)\n",
    "    subarray_1:np.ndarray = vector_or_matrix[:rows_1]\n",
    "    subarray_2:np.ndarray = vector_or_matrix[rows_1:]\n",
    "    return subarray_1, subarray_2\n",
    "\n",
    "small_matrix = np.random.rand(400, 40)\n",
    "train, test = split_train_test(small_matrix, 70)\n",
    "assert train.shape == (280, 40), f'Ar trebui sa fie forma (280, 40), dar am obtinut {train.shape}'\n",
    "assert test.shape == (120, 40), f'Ar trebui sa fie forma (280, 40), dar am obtinut {test.shape}'\n",
    "assert np.all(np.vstack((train, test)) == small_matrix)\n",
    "del small_matrix, train, test\n",
    "\n",
    "small_vector = np.random.rand(200)\n",
    "train, test = split_train_test(small_vector, 70)\n",
    "assert train.shape == (140,), f'Ar trebui sa fie forma (140,), dar am obtinut {train.shape}'\n",
    "assert test.shape == (60,), f'Ar trebui sa fie forma (60,), dar am obtinut {test.shape}'\n",
    "assert np.all(np.hstack((train, test)) == small_vector)\n",
    "del small_vector, train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3700d",
   "metadata": {},
   "source": [
    "Functia urmatoare trebuie sa preia o matrice si sa faca scalarea la intervalul [0, 1] a fiecarei coloane (trasaturi), exceptand coloanele care au valoare constanta - acestea sunt lasate nemodificate.\n",
    "\n",
    "Functia trebuie sa returneze: matricea rezultata in urma scalarii (cea data ca argument nu se va modifica) si vectorii de minime si de maxime de pe coloane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb3adec8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T18:47:37.049807Z",
     "start_time": "2023-10-22T18:47:36.787832Z"
    }
   },
   "outputs": [],
   "source": [
    "# daca parametrii min_cols si max_cols sunt specificati, atunci se vor folosi. \n",
    "# Daca nu, se vor calcula ca vectorii cu valorile minime, respectiv maxime de pe coloanele lui matrix \n",
    "def scale(column:np.ndarray, minimum:float,maximum:float)->np.ndarray :\n",
    "    \"\"\"\n",
    "    This function scales a column of a matrix to the range [0,1].\n",
    "    It returns a vertical array (column).\n",
    "    \"\"\"\n",
    "    range:float = column.max() - column.min()\n",
    "    if range == 0 :\n",
    "        return column\n",
    "    a:float = (column - column.min()) / range #scales each element in the column by subtracting the minimum and dividing by the range\n",
    "    return a\n",
    "\n",
    "def min_max_scale(matrix:np.ndarray, min_cols:np.ndarray=None, max_cols:np.ndarray=None)->tuple:\n",
    "    \"\"\"\n",
    "    This function performs Min-Max scaling on a matrix given as input.\n",
    "    It returns a tuple containing the scaled matrix, mins on columns and max on columns.\n",
    "    \"\"\"\n",
    "    if min_cols is None:\n",
    "        min_cols = matrix.min(0) # minimele pe coloanele din X \n",
    "    if max_cols is None:\n",
    "        max_cols = matrix.max(0) # maximele pe coloanele din X \n",
    "    # construieste in result matricea scalata\n",
    "    nr_cols:int = matrix.shape[1] #accesses the second element of shape, nr of columns\n",
    "    result:np.ndarray = np.empty_like(matrix)\n",
    "    for column in range(nr_cols) :\n",
    "        #extracts the entire column as a 1D array from the 2D matrix\n",
    "        result[:, column] = scale(matrix[:, column], min_cols[column], max_cols[column])\n",
    "    return result, min_cols, max_cols\n",
    "\n",
    "dummy_matrix = np.eye(100)\n",
    "res_dummy_matrix, min_cols, max_cols = min_max_scale(dummy_matrix)\n",
    "assert np.all(res_dummy_matrix == dummy_matrix)\n",
    "assert np.all(min_cols == 0)\n",
    "assert np.all(max_cols == 1)\n",
    "\n",
    "dummy_matrix = np.full((100, 30), 10)\n",
    "res_dummy_matrix, min_cols, max_cols = min_max_scale(dummy_matrix)\n",
    "assert np.all(res_dummy_matrix == dummy_matrix)\n",
    "assert np.all(min_cols == 10)\n",
    "assert np.all(max_cols == 10)\n",
    "\n",
    "dummy_matrix = np.random.randn(10000, 200) * 10\n",
    "dummy_matrix = np.hstack((dummy_matrix, np.full((10000, 4), -100.0)))\n",
    "res_dummy_matrix, min_cols, max_cols = min_max_scale(dummy_matrix)\n",
    "assert np.allclose(np.min(res_dummy_matrix, axis=0)[0:200], 0.0)\n",
    "assert np.allclose(np.max(res_dummy_matrix, axis=0)[0:200], 1.0)\n",
    "assert np.allclose(np.min(res_dummy_matrix, axis=0)[200:], -100.0)\n",
    "assert np.allclose(np.max(res_dummy_matrix, axis=0)[200:], -100.0)\n",
    "\n",
    "dummy_matrix = np.random.randn(10000, 200) * 10\n",
    "res_dummy_matrix, min_cols, max_cols = min_max_scale(dummy_matrix)\n",
    "assert np.allclose(np.min(res_dummy_matrix, axis=0), 0.0)\n",
    "assert np.allclose(np.max(res_dummy_matrix, axis=0), 1.0)\n",
    "\n",
    "del dummy_matrix, min_cols, max_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc04dc",
   "metadata": {},
   "source": [
    "Functia de mai jos adauga o coloana de 1 la o matrice, pentru a putea reprezenta si ponderea pentru termenul liber. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f30d3b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T18:47:37.118027Z",
     "start_time": "2023-10-22T18:47:37.051807Z"
    }
   },
   "outputs": [],
   "source": [
    "def augment_matrix(mat:np.ndarray)->np.ndarray:\n",
    "    \"\"\"\n",
    "    This function auguments the matrix given as input.\n",
    "    It returns a matrix.\n",
    "    \"\"\"\n",
    "    return np.hstack((np.full((mat.shape[0], 1), 1.0), mat))\n",
    "\n",
    "dummy_matrix = np.random.randn(10000, 200) * 10\n",
    "augmented_dummy_matrix = augment_matrix(dummy_matrix)\n",
    "assert augmented_dummy_matrix.shape[0] == dummy_matrix.shape[0]\n",
    "assert augmented_dummy_matrix.shape[1] == dummy_matrix.shape[1] + 1\n",
    "assert np.all(augmented_dummy_matrix[:, 1:] == dummy_matrix)\n",
    "assert np.min(augmented_dummy_matrix[:, 0]) == np.max(augmented_dummy_matrix[:, 0]) == 1\n",
    "del dummy_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd8c1ce",
   "metadata": {},
   "source": [
    "## Functii pentru antrenare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31948703",
   "metadata": {},
   "source": [
    "Functia de eroare este eroarea patratica medie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85be59cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T18:47:37.124528Z",
     "start_time": "2023-10-22T18:47:37.120069Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient(X:np.ndarray, y:np.ndarray, theta:np.ndarray)->np.ndarray:\n",
    "    m, n = X.shape\n",
    "    assert y.shape == (m, 1), f'y ar trebuie sa fie vector coloana, dar are forma {y.shape}'\n",
    "    assert theta.shape == (n, 1), f'theta ar trebuie sa fie vector coloana, dar are forma {theta.shape}'\n",
    "    # calculeaza vector de gradient\n",
    "    # vectorul de gradient ar trebui sa aiba aceeasi forma (shape) ca si theta\n",
    "    \n",
    "    #np.dot for matrix multiplication\n",
    "    #predicted values\n",
    "    y_pred:np.ndarray = np.dot(X, theta)\n",
    "    #calc error\n",
    "    error:np.ndarray = y_pred - y\n",
    "\n",
    "    grad:np.ndarray = np.dot(X.T, error) / m\n",
    "    return grad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba10464",
   "metadata": {},
   "source": [
    "Functia de eroare este mean squared error. Pentru doi vectori $\\mathbf{y}$ si $\\mathbf{\\hat{y}}$, ambii de $m$ componente, valoarea de eroare se calculeaza ca:\n",
    "$$\n",
    "mse(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{m}\\sum\\limits_{i=0}^{m-1}(y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4f55563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T18:47:37.134947Z",
     "start_time": "2023-10-22T18:47:37.128536Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse(y_true:np.ndarray, y_hat:np.ndarray)->float:\n",
    "    squared_errors:np.ndarray = (y_true - y_hat) ** 2\n",
    "    #calc mse by taking the avg from squared_errors\n",
    "    mean_squared_error:float = np.mean(squared_errors)\n",
    "    return mean_squared_error\n",
    "\n",
    "y_true = np.array([0, 1, 2, 3, 0])\n",
    "y_hat = y_true.copy()\n",
    "assert mse(y_true, y_hat) == 0\n",
    "\n",
    "y_hat = np.array([0, 0, 0, 0, 0])\n",
    "assert mse(y_true, y_hat) == 2.8\n",
    "\n",
    "y_hat = np.array([1, 2, 3, 4, 1])\n",
    "assert mse(y_true, y_hat) == 1\n",
    "\n",
    "del y_true, y_hat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7658a52e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T18:47:37.143577Z",
     "start_time": "2023-10-22T18:47:37.136956Z"
    }
   },
   "outputs": [],
   "source": [
    "# functia predict ia un set de valori de intrare si ponderile modelului \n",
    "# si calculeaza iesirea estimata de model\n",
    "\n",
    "def predict(X:np.ndarray, theta:np.ndarray)->np.ndarray:\n",
    "    #the number of columns in the feature matrix matches the number of rows in the parameter vector -> valid matrix multiplication\n",
    "    assert X.shape[1] == theta.shape[0] and theta.ndim == 2\n",
    "    y_hat:np.ndarray = np.dot(X, theta)\n",
    "    return y_hat\n",
    "\n",
    "X_dummy = np.linspace(0, 1, 100).reshape(20, 5)\n",
    "theta_dummy = np.arange(5).reshape(-1, 1)\n",
    "y_hat_dummy = predict(X_dummy, theta_dummy)\n",
    "assert np.allclose(y_hat_dummy, np.array([0.3030303,0.80808081,1.31313131,1.81818182,2.32323232,2.82828283\n",
    ",3.33333333,3.83838384,4.34343434,4.84848485,5.35353535,5.85858586\n",
    ",6.36363636,6.86868687,7.37373737,7.87878788,8.38383838,8.88888889\n",
    ",9.39393939,9.8989899]).reshape(-1, 1))\n",
    "\n",
    "theta_dummy = np.arange(10, 63, 13).reshape(-1, 1)\n",
    "y_hat_dummy = predict(X_dummy, theta_dummy)\n",
    "assert np.allclose(y_hat_dummy, np.array([4.94949495,14.04040404,23.13131313,32.22222222,41.31313131,50.4040404,59.49494949,68.58585859,77.67676768,86.76767677,95.85858586,104.94949495,114.04040404,123.13131313,132.22222222,141.31313131,150.4040404,159.49494949,168.58585859,177.67676768]).reshape(-1, 1))\n",
    "\n",
    "del X_dummy, theta_dummy, y_hat_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f23f7db6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T18:47:37.148884Z",
     "start_time": "2023-10-22T18:47:37.145599Z"
    }
   },
   "outputs": [],
   "source": [
    "# functia evalueaza cat de apropiat e vectorul de predictii fata de cel ground truth, \n",
    "# folosind functia mse\n",
    "\n",
    "def eval_model(X:np.ndarray, y:np.ndarray, theta:np.ndarray)->float:\n",
    "    m, n = X.shape\n",
    "    assert y.shape == (m, 1)\n",
    "    assert theta.shape == (n, 1)\n",
    "    y_hat:np.ndarray = predict(X,theta) \n",
    "    return mse(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2253d519",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T18:47:37.154484Z",
     "start_time": "2023-10-22T18:47:37.149857Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(X_train: np.ndarray, y_train: np.ndarray, lr: float, theta: np.ndarray) -> np.ndarray:\n",
    "    assert lr > 0, f'Learning rate should be strictly positive, but received {lr}'\n",
    "    # Determine the gradient for the current weight vector\n",
    "    grad_theta: np.ndarray = gradient(X_train, y_train, theta)\n",
    "    # Update the values in the theta vector using the gradient\n",
    "    theta: np.ndarray = theta - lr * grad_theta\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e590500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:03:31.008036Z",
     "start_time": "2023-10-22T19:03:31.002710Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(X_train: np.ndarray, y_train: np.ndarray, lr: float,\n",
    "                X_test: np.ndarray, y_test: np.ndarray,\n",
    "                n_epochs: int = 1000, log_interval: int = 10) -> tuple:\n",
    "    m, n = X_train.shape\n",
    "    assert y_train.shape == (m, 1)\n",
    "    theta: np.ndarray = np.zeros((n, 1))\n",
    "    # List to represent MSE values throughout epochs\n",
    "    mses_train: list[float] = []\n",
    "    \n",
    "    for i in range(1, n_epochs + 1):\n",
    "        theta = train_one_epoch(X_train, y_train, lr, theta)\n",
    "        mse_train: float = eval_model(X_train, y_train, theta)\n",
    "        if i % log_interval == 0:\n",
    "            mse_test: float = eval_model(X_test, y_test, theta)\n",
    "            print(f'Epoch {i}/{n_epochs}:\\n\\tmse on train set = {mse_train}\\n\\tmse on test set = {mse_test}')\n",
    "        mses_train.append(mse_train)\n",
    "    \n",
    "    return theta, mses_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b314aa",
   "metadata": {},
   "source": [
    "## Model de regresie liniara determinat cu metoda gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "330e65cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:03:32.425773Z",
     "start_time": "2023-10-22T19:03:32.395584Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = './data/Concrete_Data.xls'\n",
    "data = pd.read_excel(data_path)\n",
    "\n",
    "X:np.ndarray = data.iloc[:, :-1].values\n",
    "# y va fi un vector coloana\n",
    "y:np.ndarray = data.iloc[:, -1].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59172bab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:03:32.600320Z",
     "start_time": "2023-10-22T19:03:32.595387Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se face permutare aleatoare a setului de date\n",
    "\n",
    "X, y = random_shuffle(X, y)\n",
    "\n",
    "assert X.shape == (1030, 8) and y.shape == (1030, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e9bd196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:03:32.780592Z",
     "start_time": "2023-10-22T19:03:32.775292Z"
    }
   },
   "outputs": [],
   "source": [
    "# se face impartirea in seturi de date de train si test \n",
    "p=60\n",
    "X_train, X_test = split_train_test(X, p)\n",
    "y_train, y_test = split_train_test(y, p)\n",
    "\n",
    "assert X_train.shape[0] + X_test.shape[0] == X.shape[0] and X_train.shape[1] == X_test.shape[1] == X.shape[1]\n",
    "assert y_train.shape[0] + y_test.shape[0] == y.shape[0] \n",
    "assert X_train.shape[0] == y_train.shape[0] == int(p/100 * X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54878788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:03:32.963541Z",
     "start_time": "2023-10-22T19:03:32.960041Z"
    }
   },
   "outputs": [],
   "source": [
    "# se adauga coloana de 1 la X_train, X_test\n",
    "X_train:np.ndarray = augment_matrix(X_train)\n",
    "X_test:np.ndarray = augment_matrix(X_test)\n",
    "\n",
    "assert np.alltrue(X_train[:, 0] == 1) and np.alltrue(X_test[:, 0] == 1)\n",
    "assert X_train.shape == (618, 9), f'got shape {X_train.shape}'\n",
    "assert X_test.shape == (412, 9), f'got shape {X_test.shape}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d307c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:03:33.164897Z",
     "start_time": "2023-10-22T19:03:33.161979Z"
    }
   },
   "outputs": [],
   "source": [
    "# se face scalarea la [0, 1] a lui X_train. Vectorii de min si max pe coloane se retin \n",
    "# pentru scalarea lui X_test\n",
    "\n",
    "X_train_scaled, min_cols, max_cols = min_max_scale(X_train, min_cols=None, max_cols=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "914181a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:03:33.505269Z",
     "start_time": "2023-10-22T19:03:33.501272Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_test se va scala, dar folosind  min_cols si max_cols\n",
    "X_test_scaled, _, _ = min_max_scale(X_test, min_cols, max_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7be15e77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:03:34.116232Z",
     "start_time": "2023-10-22T19:03:34.043025Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/2000:\n",
      "\tmse on train set = 285.1122587002458\n",
      "\tmse on test set = 242.29458421779327\n",
      "Epoch 20/2000:\n",
      "\tmse on train set = 255.6594373470911\n",
      "\tmse on test set = 233.56513916349635\n",
      "Epoch 30/2000:\n",
      "\tmse on train set = 242.5037610020453\n",
      "\tmse on test set = 222.86996874506394\n",
      "Epoch 40/2000:\n",
      "\tmse on train set = 231.08707990798072\n",
      "\tmse on test set = 211.90969839159487\n",
      "Epoch 50/2000:\n",
      "\tmse on train set = 221.01685628895274\n",
      "\tmse on test set = 202.0113493671728\n",
      "Epoch 60/2000:\n",
      "\tmse on train set = 212.09597026198927\n",
      "\tmse on test set = 193.17254456201334\n",
      "Epoch 70/2000:\n",
      "\tmse on train set = 204.16223114932777\n",
      "\tmse on test set = 185.26755767690872\n",
      "Epoch 80/2000:\n",
      "\tmse on train set = 197.08035460498562\n",
      "\tmse on test set = 178.17811527934697\n",
      "Epoch 90/2000:\n",
      "\tmse on train set = 190.73690635850315\n",
      "\tmse on test set = 171.80297262529797\n",
      "Epoch 100/2000:\n",
      "\tmse on train set = 185.03629478602122\n",
      "\tmse on test set = 166.05592875844133\n",
      "Epoch 110/2000:\n",
      "\tmse on train set = 179.89757073546352\n",
      "\tmse on test set = 160.86327971636146\n",
      "Epoch 120/2000:\n",
      "\tmse on train set = 175.25186106548068\n",
      "\tmse on test set = 156.16168400076702\n",
      "Epoch 130/2000:\n",
      "\tmse on train set = 171.0403013609876\n",
      "\tmse on test set = 151.89643651159628\n",
      "Epoch 140/2000:\n",
      "\tmse on train set = 167.21236318663261\n",
      "\tmse on test set = 148.0200720901488\n",
      "Epoch 150/2000:\n",
      "\tmse on train set = 163.72449430047996\n",
      "\tmse on test set = 144.49122998827738\n",
      "Epoch 160/2000:\n",
      "\tmse on train set = 160.53900806553543\n",
      "\tmse on test set = 141.27372518822096\n",
      "Epoch 170/2000:\n",
      "\tmse on train set = 157.6231720828484\n",
      "\tmse on test set = 138.33578440870224\n",
      "Epoch 180/2000:\n",
      "\tmse on train set = 154.9484567578451\n",
      "\tmse on test set = 135.64941387159243\n",
      "Epoch 190/2000:\n",
      "\tmse on train set = 152.48991281442295\n",
      "\tmse on test set = 133.18987303575267\n",
      "Epoch 200/2000:\n",
      "\tmse on train set = 150.2256532357797\n",
      "\tmse on test set = 130.9352340172962\n",
      "Epoch 210/2000:\n",
      "\tmse on train set = 148.1364201562601\n",
      "\tmse on test set = 128.86601068548396\n",
      "Epoch 220/2000:\n",
      "\tmse on train set = 146.20522117660386\n",
      "\tmse on test set = 126.9648447392489\n",
      "Epoch 230/2000:\n",
      "\tmse on train set = 144.4170226732836\n",
      "\tmse on test set = 125.21623865146096\n",
      "Epoch 240/2000:\n",
      "\tmse on train set = 142.7584901114776\n",
      "\tmse on test set = 123.606327385133\n",
      "Epoch 250/2000:\n",
      "\tmse on train set = 141.21776729726315\n",
      "\tmse on test set = 122.12268236692012\n",
      "Epoch 260/2000:\n",
      "\tmse on train set = 139.784288030948\n",
      "\tmse on test set = 120.75414244736655\n",
      "Epoch 270/2000:\n",
      "\tmse on train set = 138.44861483745726\n",
      "\tmse on test set = 119.49066756026622\n",
      "Epoch 280/2000:\n",
      "\tmse on train set = 137.20230041897605\n",
      "\tmse on test set = 118.32321157347351\n",
      "Epoch 290/2000:\n",
      "\tmse on train set = 136.03776825203556\n",
      "\tmse on test set = 117.24361144533633\n",
      "Epoch 300/2000:\n",
      "\tmse on train set = 134.94820937662962\n",
      "\tmse on test set = 116.24449029913856\n",
      "Epoch 310/2000:\n",
      "\tmse on train set = 133.92749293047427\n",
      "\tmse on test set = 115.31917242917864\n",
      "Epoch 320/2000:\n",
      "\tmse on train set = 132.9700883918937\n",
      "\tmse on test set = 114.4616085769776\n",
      "Epoch 330/2000:\n",
      "\tmse on train set = 132.07099782940062\n",
      "\tmse on test set = 113.66631008059092\n",
      "Epoch 340/2000:\n",
      "\tmse on train set = 131.22569673001047\n",
      "\tmse on test set = 112.92829071650694\n",
      "Epoch 350/2000:\n",
      "\tmse on train set = 130.43008220364388\n",
      "\tmse on test set = 112.24301523184617\n",
      "Epoch 360/2000:\n",
      "\tmse on train set = 129.68042754705698\n",
      "\tmse on test set = 111.60635371210547\n",
      "Epoch 370/2000:\n",
      "\tmse on train set = 128.9733423050706\n",
      "\tmse on test set = 111.01454105246737\n",
      "Epoch 380/2000:\n",
      "\tmse on train set = 128.3057370953832\n",
      "\tmse on test set = 110.46414090341588\n",
      "Epoch 390/2000:\n",
      "\tmse on train set = 127.67479257069172\n",
      "\tmse on test set = 109.95201354776871\n",
      "Epoch 400/2000:\n",
      "\tmse on train set = 127.07793198199954\n",
      "\tmse on test set = 109.47528723921403\n",
      "Epoch 410/2000:\n",
      "\tmse on train set = 126.51279688292132\n",
      "\tmse on test set = 109.03133259438046\n",
      "Epoch 420/2000:\n",
      "\tmse on train set = 125.9772255789582\n",
      "\tmse on test set = 108.61773968326814\n",
      "Epoch 430/2000:\n",
      "\tmse on train set = 125.46923398011876\n",
      "\tmse on test set = 108.23229750805561\n",
      "Epoch 440/2000:\n",
      "\tmse on train set = 124.98699856152442\n",
      "\tmse on test set = 107.87297559911246\n",
      "Epoch 450/2000:\n",
      "\tmse on train set = 124.52884117609479\n",
      "\tmse on test set = 107.53790749050094\n",
      "Epoch 460/2000:\n",
      "\tmse on train set = 124.0932154971523\n",
      "\tmse on test set = 107.22537586617672\n",
      "Epoch 470/2000:\n",
      "\tmse on train set = 123.67869489771412\n",
      "\tmse on test set = 106.93379919317724\n",
      "Epoch 480/2000:\n",
      "\tmse on train set = 123.28396159810245\n",
      "\tmse on test set = 106.66171967989123\n",
      "Epoch 490/2000:\n",
      "\tmse on train set = 122.9077969349174\n",
      "\tmse on test set = 106.40779241650053\n",
      "Epoch 500/2000:\n",
      "\tmse on train set = 122.54907262289875\n",
      "\tmse on test set = 106.17077557127925\n",
      "Epoch 510/2000:\n",
      "\tmse on train set = 122.2067428971863\n",
      "\tmse on test set = 105.94952153095275\n",
      "Epoch 520/2000:\n",
      "\tmse on train set = 121.87983743733514\n",
      "\tmse on test set = 105.74296888604655\n",
      "Epoch 530/2000:\n",
      "\tmse on train set = 121.56745498646112\n",
      "\tmse on test set = 105.55013517333032\n",
      "Epoch 540/2000:\n",
      "\tmse on train set = 121.2687575893372\n",
      "\tmse on test set = 105.37011029728936\n",
      "Epoch 550/2000:\n",
      "\tmse on train set = 120.98296538235876\n",
      "\tmse on test set = 105.20205056121068\n",
      "Epoch 560/2000:\n",
      "\tmse on train set = 120.70935187622507\n",
      "\tmse on test set = 105.04517324610163\n",
      "Epoch 570/2000:\n",
      "\tmse on train set = 120.44723967911095\n",
      "\tmse on test set = 104.89875168239712\n",
      "Epoch 580/2000:\n",
      "\tmse on train set = 120.19599661415586\n",
      "\tmse on test set = 104.76211076536556\n",
      "Epoch 590/2000:\n",
      "\tmse on train set = 119.95503219040013\n",
      "\tmse on test set = 104.63462287039374\n",
      "Epoch 600/2000:\n",
      "\tmse on train set = 119.72379439094436\n",
      "\tmse on test set = 104.51570412899754\n",
      "Epoch 610/2000:\n",
      "\tmse on train set = 119.50176674618764\n",
      "\tmse on test set = 104.4048110305433\n",
      "Epoch 620/2000:\n",
      "\tmse on train set = 119.28846566358337\n",
      "\tmse on test set = 104.30143731833712\n",
      "Epoch 630/2000:\n",
      "\tmse on train set = 119.08343798850657\n",
      "\tmse on test set = 104.2051111520012\n",
      "Epoch 640/2000:\n",
      "\tmse on train set = 118.88625877360322\n",
      "\tmse on test set = 104.11539251095675\n",
      "Epoch 650/2000:\n",
      "\tmse on train set = 118.69652923644212\n",
      "\tmse on test set = 104.03187081641286\n",
      "Epoch 660/2000:\n",
      "\tmse on train set = 118.51387488745276\n",
      "\tmse on test set = 103.95416275155988\n",
      "Epoch 670/2000:\n",
      "\tmse on train set = 118.33794381204385\n",
      "\tmse on test set = 103.88191026171413\n",
      "Epoch 680/2000:\n",
      "\tmse on train set = 118.16840509248918\n",
      "\tmse on test set = 103.81477871798803\n",
      "Epoch 690/2000:\n",
      "\tmse on train set = 118.00494735666487\n",
      "\tmse on test set = 103.75245522969223\n",
      "Epoch 700/2000:\n",
      "\tmse on train set = 117.84727744205236\n",
      "\tmse on test set = 103.69464709213354\n",
      "Epoch 710/2000:\n",
      "\tmse on train set = 117.69511916460026\n",
      "\tmse on test set = 103.64108035777713\n",
      "Epoch 720/2000:\n",
      "\tmse on train set = 117.54821218308682\n",
      "\tmse on test set = 103.59149851990833\n",
      "Epoch 730/2000:\n",
      "\tmse on train set = 117.40631095055923\n",
      "\tmse on test set = 103.54566129897447\n",
      "Epoch 740/2000:\n",
      "\tmse on train set = 117.26918374525515\n",
      "\tmse on test set = 103.5033435227255\n",
      "Epoch 750/2000:\n",
      "\tmse on train set = 117.13661177415548\n",
      "\tmse on test set = 103.46433409211177\n",
      "Epoch 760/2000:\n",
      "\tmse on train set = 117.00838834297737\n",
      "\tmse on test set = 103.42843502565326\n",
      "Epoch 770/2000:\n",
      "\tmse on train set = 116.88431808700939\n",
      "\tmse on test set = 103.39546057567352\n",
      "Epoch 780/2000:\n",
      "\tmse on train set = 116.76421625771903\n",
      "\tmse on test set = 103.36523641040074\n",
      "Epoch 790/2000:\n",
      "\tmse on train set = 116.64790806053851\n",
      "\tmse on test set = 103.33759885648891\n",
      "Epoch 800/2000:\n",
      "\tmse on train set = 116.53522803965765\n",
      "\tmse on test set = 103.31239419700697\n",
      "Epoch 810/2000:\n",
      "\tmse on train set = 116.4260195060376\n",
      "\tmse on test set = 103.28947802038923\n",
      "Epoch 820/2000:\n",
      "\tmse on train set = 116.32013400519882\n",
      "\tmse on test set = 103.26871461624364\n",
      "Epoch 830/2000:\n",
      "\tmse on train set = 116.21743082164868\n",
      "\tmse on test set = 103.24997641427865\n",
      "Epoch 840/2000:\n",
      "\tmse on train set = 116.11777651708968\n",
      "\tmse on test set = 103.23314346293688\n",
      "Epoch 850/2000:\n",
      "\tmse on train set = 116.02104449980163\n",
      "\tmse on test set = 103.21810294462232\n",
      "Epoch 860/2000:\n",
      "\tmse on train set = 115.92711462281656\n",
      "\tmse on test set = 103.20474872467553\n",
      "Epoch 870/2000:\n",
      "\tmse on train set = 115.83587280871029\n",
      "\tmse on test set = 103.19298093149708\n",
      "Epoch 880/2000:\n",
      "\tmse on train set = 115.74721069901905\n",
      "\tmse on test set = 103.18270556543831\n",
      "Epoch 890/2000:\n",
      "\tmse on train set = 115.66102532645758\n",
      "\tmse on test set = 103.17383413428084\n",
      "Epoch 900/2000:\n",
      "\tmse on train set = 115.57721880826638\n",
      "\tmse on test set = 103.16628331330728\n",
      "Epoch 910/2000:\n",
      "\tmse on train set = 115.49569805915496\n",
      "\tmse on test set = 103.15997462813202\n",
      "Epoch 920/2000:\n",
      "\tmse on train set = 115.41637452243071\n",
      "\tmse on test set = 103.15483415861068\n",
      "Epoch 930/2000:\n",
      "\tmse on train set = 115.33916391801937\n",
      "\tmse on test set = 103.15079226228465\n",
      "Epoch 940/2000:\n",
      "\tmse on train set = 115.26398600618525\n",
      "\tmse on test set = 103.14778331594128\n",
      "Epoch 950/2000:\n",
      "\tmse on train set = 115.19076436585347\n",
      "\tmse on test set = 103.14574547398486\n",
      "Epoch 960/2000:\n",
      "\tmse on train set = 115.11942618652368\n",
      "\tmse on test set = 103.14462044241618\n",
      "Epoch 970/2000:\n",
      "\tmse on train set = 115.04990207284224\n",
      "\tmse on test set = 103.14435326731493\n",
      "Epoch 980/2000:\n",
      "\tmse on train set = 114.98212586097164\n",
      "\tmse on test set = 103.14489213680388\n",
      "Epoch 990/2000:\n",
      "\tmse on train set = 114.91603444596242\n",
      "\tmse on test set = 103.14618819555471\n",
      "Epoch 1000/2000:\n",
      "\tmse on train set = 114.85156761939187\n",
      "\tmse on test set = 103.14819537096625\n",
      "Epoch 1010/2000:\n",
      "\tmse on train set = 114.78866791658858\n",
      "\tmse on test set = 103.15087021021341\n",
      "Epoch 1020/2000:\n",
      "\tmse on train set = 114.72728047281429\n",
      "\tmse on test set = 103.15417172742563\n",
      "Epoch 1030/2000:\n",
      "\tmse on train set = 114.6673528878176\n",
      "\tmse on test set = 103.15806126030812\n",
      "Epoch 1040/2000:\n",
      "\tmse on train set = 114.60883509821979\n",
      "\tmse on test set = 103.1625023355728\n",
      "Epoch 1050/2000:\n",
      "\tmse on train set = 114.55167925722921\n",
      "\tmse on test set = 103.16746054258996\n",
      "Epoch 1060/2000:\n",
      "\tmse on train set = 114.49583962121913\n",
      "\tmse on test set = 103.17290341471698\n",
      "Epoch 1070/2000:\n",
      "\tmse on train set = 114.44127244273464\n",
      "\tmse on test set = 103.1788003177989\n",
      "Epoch 1080/2000:\n",
      "\tmse on train set = 114.3879358695266\n",
      "\tmse on test set = 103.18512234537287\n",
      "Epoch 1090/2000:\n",
      "\tmse on train set = 114.33578984923702\n",
      "\tmse on test set = 103.19184222014088\n",
      "Epoch 1100/2000:\n",
      "\tmse on train set = 114.28479603938828\n",
      "\tmse on test set = 103.19893420130809\n",
      "Epoch 1110/2000:\n",
      "\tmse on train set = 114.23491772234922\n",
      "\tmse on test set = 103.20637399741038\n",
      "Epoch 1120/2000:\n",
      "\tmse on train set = 114.18611972497729\n",
      "\tmse on test set = 103.21413868428279\n",
      "Epoch 1130/2000:\n",
      "\tmse on train set = 114.13836834265281\n",
      "\tmse on test set = 103.22220662784385\n",
      "Epoch 1140/2000:\n",
      "\tmse on train set = 114.09163126744215\n",
      "\tmse on test set = 103.23055741139346\n",
      "Epoch 1150/2000:\n",
      "\tmse on train set = 114.04587752014393\n",
      "\tmse on test set = 103.23917176714308\n",
      "Epoch 1160/2000:\n",
      "\tmse on train set = 114.0010773859881\n",
      "\tmse on test set = 103.24803151171525\n",
      "Epoch 1170/2000:\n",
      "\tmse on train set = 113.95720235377306\n",
      "\tmse on test set = 103.25711948536825\n",
      "Epoch 1180/2000:\n",
      "\tmse on train set = 113.91422505823985\n",
      "\tmse on test set = 103.2664194947175\n",
      "Epoch 1190/2000:\n",
      "\tmse on train set = 113.87211922549531\n",
      "\tmse on test set = 103.27591625874062\n",
      "Epoch 1200/2000:\n",
      "\tmse on train set = 113.8308596213086\n",
      "\tmse on test set = 103.28559535786694\n",
      "Epoch 1210/2000:\n",
      "\tmse on train set = 113.79042200211528\n",
      "\tmse on test set = 103.29544318596614\n",
      "Epoch 1220/2000:\n",
      "\tmse on train set = 113.75078306857588\n",
      "\tmse on test set = 103.30544690506161\n",
      "Epoch 1230/2000:\n",
      "\tmse on train set = 113.7119204215428\n",
      "\tmse on test set = 103.31559440260665\n",
      "Epoch 1240/2000:\n",
      "\tmse on train set = 113.67381252030042\n",
      "\tmse on test set = 103.3258742511708\n",
      "Epoch 1250/2000:\n",
      "\tmse on train set = 113.63643864295118\n",
      "\tmse on test set = 103.3362756703948\n",
      "Epoch 1260/2000:\n",
      "\tmse on train set = 113.59977884882719\n",
      "\tmse on test set = 103.34678849107993\n",
      "Epoch 1270/2000:\n",
      "\tmse on train set = 113.56381394281591\n",
      "\tmse on test set = 103.35740312128809\n",
      "Epoch 1280/2000:\n",
      "\tmse on train set = 113.52852544149374\n",
      "\tmse on test set = 103.36811051433416\n",
      "Epoch 1290/2000:\n",
      "\tmse on train set = 113.49389554096847\n",
      "\tmse on test set = 103.37890213856238\n",
      "Epoch 1300/2000:\n",
      "\tmse on train set = 113.45990708633741\n",
      "\tmse on test set = 103.38976994880264\n",
      "Epoch 1310/2000:\n",
      "\tmse on train set = 113.42654354267333\n",
      "\tmse on test set = 103.40070635941112\n",
      "Epoch 1320/2000:\n",
      "\tmse on train set = 113.39378896745555\n",
      "\tmse on test set = 103.4117042188035\n",
      "Epoch 1330/2000:\n",
      "\tmse on train set = 113.36162798436862\n",
      "\tmse on test set = 103.4227567853967\n",
      "Epoch 1340/2000:\n",
      "\tmse on train set = 113.3300457583952\n",
      "\tmse on test set = 103.43385770487855\n",
      "Epoch 1350/2000:\n",
      "\tmse on train set = 113.29902797213418\n",
      "\tmse on test set = 103.44500098873024\n",
      "Epoch 1360/2000:\n",
      "\tmse on train set = 113.26856080327923\n",
      "\tmse on test set = 103.45618099393113\n",
      "Epoch 1370/2000:\n",
      "\tmse on train set = 113.23863090319612\n",
      "\tmse on test set = 103.46739240377936\n",
      "Epoch 1380/2000:\n",
      "\tmse on train set = 113.20922537654141\n",
      "\tmse on test set = 103.47863020976553\n",
      "Epoch 1390/2000:\n",
      "\tmse on train set = 113.18033176186772\n",
      "\tmse on test set = 103.48988969444115\n",
      "Epoch 1400/2000:\n",
      "\tmse on train set = 113.1519380131642\n",
      "\tmse on test set = 103.5011664152258\n",
      "Epoch 1410/2000:\n",
      "\tmse on train set = 113.12403248228374\n",
      "\tmse on test set = 103.51245618910153\n",
      "Epoch 1420/2000:\n",
      "\tmse on train set = 113.09660390221086\n",
      "\tmse on test set = 103.52375507814487\n",
      "Epoch 1430/2000:\n",
      "\tmse on train set = 113.06964137112706\n",
      "\tmse on test set = 103.53505937585031\n",
      "Epoch 1440/2000:\n",
      "\tmse on train set = 113.04313433723289\n",
      "\tmse on test set = 103.54636559420194\n",
      "Epoch 1450/2000:\n",
      "\tmse on train set = 113.01707258428753\n",
      "\tmse on test set = 103.55767045145183\n",
      "Epoch 1460/2000:\n",
      "\tmse on train set = 112.99144621783009\n",
      "\tmse on test set = 103.56897086056631\n",
      "Epoch 1470/2000:\n",
      "\tmse on train set = 112.96624565204729\n",
      "\tmse on test set = 103.58026391830377\n",
      "Epoch 1480/2000:\n",
      "\tmse on train set = 112.94146159725541\n",
      "\tmse on test set = 103.59154689488989\n",
      "Epoch 1490/2000:\n",
      "\tmse on train set = 112.91708504796503\n",
      "\tmse on test set = 103.60281722425657\n",
      "Epoch 1500/2000:\n",
      "\tmse on train set = 112.89310727149989\n",
      "\tmse on test set = 103.61407249481536\n",
      "Epoch 1510/2000:\n",
      "\tmse on train set = 112.86951979714159\n",
      "\tmse on test set = 103.6253104407351\n",
      "Epoch 1520/2000:\n",
      "\tmse on train set = 112.84631440577441\n",
      "\tmse on test set = 103.63652893369726\n",
      "Epoch 1530/2000:\n",
      "\tmse on train set = 112.82348312000472\n",
      "\tmse on test set = 103.64772597510249\n",
      "Epoch 1540/2000:\n",
      "\tmse on train set = 112.80101819473238\n",
      "\tmse on test set = 103.65889968870464\n",
      "Epoch 1550/2000:\n",
      "\tmse on train set = 112.77891210815092\n",
      "\tmse on test set = 103.67004831364837\n",
      "Epoch 1560/2000:\n",
      "\tmse on train set = 112.75715755315584\n",
      "\tmse on test set = 103.68117019788916\n",
      "Epoch 1570/2000:\n",
      "\tmse on train set = 112.7357474291411\n",
      "\tmse on test set = 103.69226379197505\n",
      "Epoch 1580/2000:\n",
      "\tmse on train set = 112.71467483416454\n",
      "\tmse on test set = 103.70332764317018\n",
      "Epoch 1590/2000:\n",
      "\tmse on train set = 112.69393305746432\n",
      "\tmse on test set = 103.71436038990208\n",
      "Epoch 1600/2000:\n",
      "\tmse on train set = 112.67351557230919\n",
      "\tmse on test set = 103.72536075651553\n",
      "Epoch 1610/2000:\n",
      "\tmse on train set = 112.65341602916699\n",
      "\tmse on test set = 103.7363275483157\n",
      "Epoch 1620/2000:\n",
      "\tmse on train set = 112.63362824917495\n",
      "\tmse on test set = 103.74725964688601\n",
      "Epoch 1630/2000:\n",
      "\tmse on train set = 112.61414621789831\n",
      "\tmse on test set = 103.75815600566509\n",
      "Epoch 1640/2000:\n",
      "\tmse on train set = 112.59496407936275\n",
      "\tmse on test set = 103.76901564576937\n",
      "Epoch 1650/2000:\n",
      "\tmse on train set = 112.57607613034756\n",
      "\tmse on test set = 103.77983765204817\n",
      "Epoch 1660/2000:\n",
      "\tmse on train set = 112.55747681492751\n",
      "\tmse on test set = 103.79062116935854\n",
      "Epoch 1670/2000:\n",
      "\tmse on train set = 112.539160719251\n",
      "\tmse on test set = 103.80136539904791\n",
      "Epoch 1680/2000:\n",
      "\tmse on train set = 112.52112256654371\n",
      "\tmse on test set = 103.81206959563394\n",
      "Epoch 1690/2000:\n",
      "\tmse on train set = 112.50335721232679\n",
      "\tmse on test set = 103.82273306367054\n",
      "Epoch 1700/2000:\n",
      "\tmse on train set = 112.48585963983957\n",
      "\tmse on test set = 103.83335515478996\n",
      "Epoch 1710/2000:\n",
      "\tmse on train set = 112.46862495565722\n",
      "\tmse on test set = 103.84393526491179\n",
      "Epoch 1720/2000:\n",
      "\tmse on train set = 112.45164838549397\n",
      "\tmse on test set = 103.85447283160988\n",
      "Epoch 1730/2000:\n",
      "\tmse on train set = 112.43492527018333\n",
      "\tmse on test set = 103.86496733162808\n",
      "Epoch 1740/2000:\n",
      "\tmse on train set = 112.41845106182734\n",
      "\tmse on test set = 103.8754182785377\n",
      "Epoch 1750/2000:\n",
      "\tmse on train set = 112.4022213201062\n",
      "\tmse on test set = 103.8858252205283\n",
      "Epoch 1760/2000:\n",
      "\tmse on train set = 112.3862317087416\n",
      "\tmse on test set = 103.89618773832478\n",
      "Epoch 1770/2000:\n",
      "\tmse on train set = 112.37047799210633\n",
      "\tmse on test set = 103.90650544322418\n",
      "Epoch 1780/2000:\n",
      "\tmse on train set = 112.3549560319732\n",
      "\tmse on test set = 103.91677797524547\n",
      "Epoch 1790/2000:\n",
      "\tmse on train set = 112.33966178439728\n",
      "\tmse on test set = 103.92700500138608\n",
      "Epoch 1800/2000:\n",
      "\tmse on train set = 112.32459129672498\n",
      "\tmse on test set = 103.9371862139799\n",
      "Epoch 1810/2000:\n",
      "\tmse on train set = 112.30974070472418\n",
      "\tmse on test set = 103.94732132915041\n",
      "Epoch 1820/2000:\n",
      "\tmse on train set = 112.29510622983032\n",
      "\tmse on test set = 103.95741008535458\n",
      "Epoch 1830/2000:\n",
      "\tmse on train set = 112.2806841765023\n",
      "\tmse on test set = 103.96745224201187\n",
      "Epoch 1840/2000:\n",
      "\tmse on train set = 112.26647092968417\n",
      "\tmse on test set = 103.9774475782144\n",
      "Epoch 1850/2000:\n",
      "\tmse on train set = 112.2524629523672\n",
      "\tmse on test set = 103.987395891513\n",
      "Epoch 1860/2000:\n",
      "\tmse on train set = 112.23865678324793\n",
      "\tmse on test set = 103.9972969967757\n",
      "Epoch 1870/2000:\n",
      "\tmse on train set = 112.2250490344778\n",
      "\tmse on test set = 104.00715072511426\n",
      "Epoch 1880/2000:\n",
      "\tmse on train set = 112.2116363895009\n",
      "\tmse on test set = 104.01695692287502\n",
      "Epoch 1890/2000:\n",
      "\tmse on train set = 112.19841560097439\n",
      "\tmse on test set = 104.02671545069076\n",
      "Epoch 1900/2000:\n",
      "\tmse on train set = 112.18538348876983\n",
      "\tmse on test set = 104.03642618258979\n",
      "Epoch 1910/2000:\n",
      "\tmse on train set = 112.17253693805027\n",
      "\tmse on test set = 104.04608900515944\n",
      "Epoch 1920/2000:\n",
      "\tmse on train set = 112.15987289742053\n",
      "\tmse on test set = 104.0557038167607\n",
      "Epoch 1930/2000:\n",
      "\tmse on train set = 112.14738837714702\n",
      "\tmse on test set = 104.06527052679098\n",
      "Epoch 1940/2000:\n",
      "\tmse on train set = 112.13508044744471\n",
      "\tmse on test set = 104.07478905499303\n",
      "Epoch 1950/2000:\n",
      "\tmse on train set = 112.12294623682715\n",
      "\tmse on test set = 104.08425933080643\n",
      "Epoch 1960/2000:\n",
      "\tmse on train set = 112.1109829305181\n",
      "\tmse on test set = 104.09368129275964\n",
      "Epoch 1970/2000:\n",
      "\tmse on train set = 112.099187768921\n",
      "\tmse on test set = 104.10305488790088\n",
      "Epoch 1980/2000:\n",
      "\tmse on train set = 112.08755804614452\n",
      "\tmse on test set = 104.11238007126462\n",
      "Epoch 1990/2000:\n",
      "\tmse on train set = 112.07609110858107\n",
      "\tmse on test set = 104.1216568053722\n",
      "Epoch 2000/2000:\n",
      "\tmse on train set = 112.06478435353691\n",
      "\tmse on test set = 104.1308850597648\n"
     ]
    }
   ],
   "source": [
    "lr:float = 0.1\n",
    "n_epochs:int = 2000\n",
    "theta, mses_train = train_model(X_train_scaled, y_train, lr, X_test_scaled, y_test, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "397d31f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:03:43.014955Z",
     "start_time": "2023-10-22T19:03:42.884050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHFCAYAAAAT5Oa6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABQ40lEQVR4nO3deVxUVeMG8GeAYdhHFmFEEdFcUtHMTEFLc0HNrTR9XdM0tTSV0jIzFcqkqLR+mUu9GuaSZa9alqmYRotL5FKKZpm4pYgassjOnN8fOBcuAwoGngvzfD+f+cDce+bOOXOHmYdzzr1XJ4QQICIiIrJhdrIrQERERCQbAxERERHZPAYiIiIisnkMRERERGTzGIiIiIjI5jEQERERkc1jICIiIiKbx0BERERENo+BiIiIiGweA1ENtH//fjz66KOoX78+DAYD/Pz8EBISgunTp8uu2k2dPn0aOp0OMTEx5Sqn0+kQERFRapmxY8cqZYrLy8vD8uXL0a5dO3h5ecHFxQWBgYEYMGAANm3aVOpzlHYr63m1IDMzExEREfjuu+9kV+W2LVmy5Jbvg1sZM2YMGjRocNuP79KlC7p06fKv6lDVFixYgM2bN8uuhibt2rULY8eORbNmzeDq6oq6detiwIABOHDgQKnlDx48iO7du8PNzQ21atXCwIEDcerUqVLLvvfee2jWrBkMBgOCgoIQGRmJvLw8q3LJyckYM2YMfHx84OLigpCQEHz77beV2k5ZIiIirD5fqzsGohrm66+/RmhoKNLS0hAdHY0dO3bg3XffRceOHfHpp5/Krl6lcnd3R0xMDMxms2p5RkYGNmzYAA8PD6vHjBo1ClOmTMFDDz2ENWvWYMuWLXj55Zfh4OCA7du3W5WfMmUK9u7da3V78sknq6xd/1ZmZiYiIyNtPhDZAgaisi1duhSnT5/GtGnTsHXrVrz77rtITk5Ghw4dsGvXLlXZ33//HV26dEFubi4+++wzrFy5En/88QceeOABXL58WVX2tddew7Rp0zBw4EBs374dkyZNwoIFCzB58mRVuZycHHTr1g3ffvst3n33XXzxxRfw8/NDr169EBcXV+Xtr2pPPvkk9u7dK7salUtQjfLggw+KRo0aiby8PKt1BQUFd7Qu169fr1D5xMREAUB89NFH5Sr35JNPCgBix44dqvX//e9/hbOzsxg5cqQo/hY/deqUACDmzp1b6naLvz6W53jzzTcr1AYtuHz5sgAg5s2bV67yFd1Pd0KLFi1E586d/9U2Ro8eLQIDA2/78Z07d/7Xdahqrq6uYvTo0eUqm5mZKcxmc9VW6DZkZmZWyXYvXbpktSw9PV34+fmJbt26qZYPHjxY+Pj4iNTUVGXZ6dOnhV6vFy+88IKy7MqVK8LJyUlMmDBB9fjXXntN6HQ6kZCQoCx7//33BQCxZ88eZVleXp5o3ry5uP/++/91+6jysYeohrl69Sp8fHzg4OBgtc7Oznp3r1u3DiEhIXBzc4ObmxvuuecerFixQlVm5cqVaN26NZycnODl5YVHH30Ux48fV5UZM2YM3NzccOTIEYSFhcHd3R3dunUDADRo0ABjxoyxeu5/OyTRtGlThIaGYuXKlVb1HThwIIxGo2r51atXAQB16tQpdXulvT63y9KdfOjQIQwcOBAeHh4wGo0YOXKk1X+cAPDpp58iJCQErq6ucHNzQ8+ePXHo0CFVGctrfPLkSTz88MNwc3NDQEAApk+fjpycHACFQ321a9cGAERGRipDfJbX31KvgwcP4rHHHoOnpycaNWoEoOz9Ud6hp127dqFLly7w9vaGs7Mz6tevj0GDBiEzM1Mpk5ubi/nz5yvDDbVr18YTTzyhek0aNGiAhIQExMXFKfW3PH9MTAx0Oh1Onz6teu7vvvsOOp3utnrFhBCIjo5GYGAgnJyccO+99+Kbb74ptWxaWhpmzJiBoKAgODo6om7duggPD8f169dv+TyxsbEYMGAA6tWrBycnJ9x1112YOHEirly5oipn2UcJCQkYNmwYjEYj/Pz8MHbsWKSmpirldDodrl+/jlWrVimvk2X/WV6nHTt2YOzYsahduzZcXFyU90llvd8sIiMj0b59e3h5ecHDwwP33nsvVqxYAVHi2uENGjRA3759sXHjRrRp0wZOTk6IjIwEACQlJWHixImoV68eHB0dlaGo/Pz8W762pfH19bVa5ubmhubNm+PcuXPKsvz8fHz11VcYNGiQqlc5MDAQDz30kGoofdu2bcjOzsYTTzyh2u4TTzwBIYSqt27Tpk1o2rQpQkJClGUODg4YOXIkfv75Z/z999+3bMPOnTvRrVs3eHh4wMXFBR07drQacqvIZ43ZbEZ0dLTy9+fr64vHH38c58+ft3rubdu2oVu3bjAajXBxccHdd9+NqKgoq+etSRiIapiQkBDs378fU6dOxf79+0sd17aYO3cuRowYAX9/f8TExGDTpk0YPXo0zpw5o5SJiorCuHHj0KJFC2zcuBHvvvsufvvtN4SEhODPP/9UbS83Nxf9+/dH165d8cUXXygfdFVp3Lhx2Lx5M1JSUgAAJ06cwJ49ezBu3DirsnfffTdq1aqFyMhIfPDBB1ZfqqUxm83Iz8+3upXXo48+irvuuguff/45IiIisHnzZvTs2VO1XxYsWIBhw4ahefPm+Oyzz7B69Wqkp6fjgQcewLFjx1Tby8vLQ//+/dGtWzd88cUXGDt2LBYtWoQ33ngDQGHY27Ztm/LaWIb45syZo9rOwIEDcdddd2HDhg1YtmxZudtTltOnT6NPnz5wdHTEypUrsW3bNrz++utwdXVFbm4ugMLXcsCAAXj99dcxfPhwfP3113j99dcRGxuLLl26ICsrC0DhF0nDhg3Rpk0bpf7Fv5QqW2RkJGbOnIkePXpg8+bNePrppzF+/HicOHFCVS4zMxOdO3fGqlWrMHXqVHzzzTeYOXMmYmJi0L9/f6sv/5L++usvhISEYOnSpdixYwfmzp2L/fv3o1OnTqX+nQ4aNAhNmjTB//73P7z44otYt24dnn32WWX93r174ezsjIcfflh5nZYsWaLaxtixY6HX67F69Wp8/vnn0Ov1lfp+szh9+jQmTpyIzz77DBs3bsTAgQMxZcoUvPrqq1btOnjwIJ5//nlMnToV27Ztw6BBg5CUlIT7778f27dvx9y5c/HNN99g3LhxiIqKwvjx42/6ulZEamoqDh48iBYtWijL/vrrL2RlZaFVq1ZW5Vu1aoWTJ08iOzsbAHD06FEAQHBwsKpcnTp14OPjo6y3lC1rmwCQkJBw07quWbMGYWFh8PDwwKpVq/DZZ5/By8sLPXv2LHUeUnk+a55++mnlvf7ll1/i1VdfxbZt2xAaGqoK5itWrMDDDz8Ms9mMZcuWYcuWLZg6dWqpwalGkdtBRZXtypUrolOnTgKAACD0er0IDQ0VUVFRIj09XSl36tQpYW9vL0aMGFHmtlJSUoSzs7N4+OGHVcvPnj0rDAaDGD58uLJs9OjRAoBYuXKl1XYCAwNL7dYvOSRR0SGzN998U6Snpws3NzexePFiIYQQzz//vAgKChJms1lMnjxZlHyLf/3118LHx0d5fby9vcXgwYPFl19+WepzlHX74YcfblrHefPmCQDi2WefVS1fu3atACDWrFkjhCh8LR0cHMSUKVNU5dLT04XJZBJDhgxRllle488++0xV9uGHHxZNmzZV7t9syMxSr9KGDcsaIirP0NPnn38uAIjDhw+XWeaTTz4RAMT//vc/1fL4+HgBQCxZskRZVtaQ2UcffSQAiMTERNXy3bt3CwBi9+7dFap3SkqKcHJyEo8++qhq+U8//SQAqOoQFRUl7OzsRHx8vKqspe1bt2696XMVZzabRV5enjhz5owAIL744gtlnWUfRUdHqx4zadIk4eTkpBr2KmvIzPI6Pf7446rlVfF+K6mgoEDk5eWJV155RXh7e6vqGxgYKOzt7cWJEydUj5k4caJwc3MTZ86cUS1/6623BADVUNS/MWLECOHg4CB++eUXZZllX3/yySdW5RcsWCAAiAsXLgghhBg/frwwGAylbrtJkyYiLCxMua/X68XEiROtyu3Zs0cAEOvWrSuzntevXxdeXl6iX79+quUFBQWidevWqiG38n7WHD9+XAAQkyZNUpXbv3+/ACBeeuklIUThe8HDw0N06tTppkOsluetSdhDVMN4e3vjhx9+QHx8PF5//XUMGDAAf/zxB2bNmoXg4GDlv4DY2FgUFBRYTQQsbu/evcjKyrIa7goICEDXrl1L/S9l0KBBldqeW3Fzc8PgwYOxcuVK5Ofn4+OPP8YTTzxRZlfuww8/jLNnz2LTpk2YMWMGWrRogc2bN6N///545plnrMpPmzYN8fHxVrd77rmnXPUbMWKE6v6QIUPg4OCA3bt3AwC2b9+O/Px8PP7446oeKCcnJ3Tu3NlqCEin06Ffv36qZa1atVL16pVHZe+ne+65B46OjpgwYQJWrVpV6tE5X331FWrVqoV+/fqp2nrPPffAZDJJmQS+d+9eZGdnW+2n0NBQBAYGqpZ99dVXaNmyJe655x5V/Xv27Fmu4brk5GQ89dRTCAgIgIODA/R6vfIcJYegAaB///6q+61atUJ2djaSk5PL3b6S+7mq3m+7du1C9+7dYTQaYW9vD71ej7lz5+Lq1atW9W3VqhWaNGmiWvbVV1/hoYcegr+/v6pevXv3BoBKmYQ8Z84crF27FosWLULbtm2t1t9s+Kf4uvKWq2jZ4vbs2YN//vkHo0ePVr0eZrMZvXr1Qnx8vNUw7a0+ayw/S36e33///bj77ruVz/M9e/YgLS0NkyZNqnFDYrdiPdGEaoT77rsP9913H4DCbu+ZM2di0aJFiI6ORnR0tDK2XK9evTK3cbM5N/7+/oiNjVUtc3FxKfXIrqo2btw4dOrUCa+99houX75c6nyl4pydnfHII4/gkUceAQCcPXsWvXv3xvvvv4+nn35a1Z1er1495XW8HSaTSXXfwcEB3t7eymt76dIlAEC7du1KfXzJeU0uLi5wcnJSLTMYDEqXfnmVNY/qdjVq1Ag7d+5EdHQ0Jk+ejOvXr6Nhw4aYOnUqpk2bBqCwrdeuXYOjo2Op2yg5l+ZOsOyHkvuptGWXLl3CyZMnodfrS93WzepvNpsRFhaGCxcuYM6cOQgODoarqyvMZjM6dOigDBcW5+3trbpvMBgAoNSyZSm5n6vi/fbzzz8jLCwMXbp0wYcffqjMAdq8eTNee+01q/qW9t67dOkStmzZcluvbXlERkZi/vz5eO2116z+8bG8zpb3QnH//PMPdDodatWqpZTNzs5GZmYmXFxcrMoWD1rF/85LlgMALy+vMutr2U+PPfZYmWX++ecfuLq6Kvdv9Vlzq89zS8gtz3dDTcVAZAP0ej3mzZuHRYsWKWPclom358+fR0BAQKmPs3xQXLx40WrdhQsX4OPjo1pW1n8TTk5OVpMwgcIPuZLbuB0dO3ZE06ZN8corr6BHjx5ltqcs9evXx4QJExAeHo6EhARVIPq3kpKSULduXeV+fn4+rl69qry2lvZ//vnnVj0SVam0feXk5KSatGtR3i+jBx54AA888AAKCgrwyy+/4L333kN4eDj8/PwwdOhQ+Pj4wNvbW5njVJK7u/stn8Py5Vzy/XS7X5iW/ZCUlGS1LikpSTWZ3MfHB87OzlaT+IuvL8vRo0fx66+/IiYmBqNHj1aWnzx58rbqXV4l93NVvN/Wr18PvV6Pr776ShWeyjodQGnvPR8fH7Rq1QqvvfZaqY/x9/e/7fpFRkYiIiICEREReOmll6zWN2rUCM7Ozjhy5IjVuiNHjuCuu+5S2mWZO3TkyBG0b99eKZeUlIQrV66gZcuWyrLg4OAytwlAVbYky35677330KFDh1LL+Pn5qe7f6rOm+Od5ybBT/PO8+HeDrWEgqmEuXrxY6n8Ali55ywdLWFgY7O3tsXTpUtVREMWFhITA2dkZa9asweDBg5Xl58+fx65du27630txDRo0wG+//aZa9scff+DEiROVEogA4OWXX8bnn39+0yHA9PR06HQ6uLm5Wa0r+fpUlrVr16r+a/zss8+Qn5+vHA3Us2dPODg44K+//qq0Yazb6UkACvfThg0bkJOTo2zj6tWr2LNnT4V6/uzt7dG+fXs0a9YMa9euxcGDBzF06FD07dsX69evR0FBgerLpKw2lFZ/S0D57bff0LRpU2X5l19+We76FdehQwc4OTlh7dq1qtd/z549OHPmjCoQ9e3bFwsWLIC3tzeCgoIq9DyWEGB5XS2WL19+W/W2KOt1KktVvN90Oh0cHBxgb2+vLMvKysLq1avLvY2+ffti69ataNSoETw9PSulXgDw6quvIiIiAi+//DLmzZtXahkHBwf069cPGzduRHR0tBLMz549i927d6smsvfq1QtOTk6IiYlRvYctR/VZep2BwknOkyZNwv79+5Wy+fn5WLNmDdq3b3/Tz5qOHTuiVq1aOHbsWKlD+aW51WdN165dARRO1i7eQxgfH4/jx49j9uzZAAqHi41GI5YtW4ahQ4fa1LAZA1EN07NnT9SrVw/9+vVDs2bNYDabcfjwYbz99ttwc3NThi8aNGiAl156Ca+++iqysrKUw3uPHTuGK1euIDIyErVq1cKcOXPw0ksv4fHHH8ewYcNw9epVREZGwsnJqcwPmJJGjRqFkSNHYtKkSRg0aBDOnDmD6Oho5T+RyjBy5EiMHDnypmVOnDiBnj17YujQoejcuTPq1KmDlJQUfP311/jggw/QpUsXhIaGqh5z9uxZ7Nu3z2pbtWvXVg5Xv5mNGzfCwcEBPXr0QEJCAubMmYPWrVtjyJAhAAr3wyuvvILZs2fj1KlT6NWrFzw9PXHp0iX8/PPPcHV1rfDReu7u7ggMDMQXX3yBbt26wcvLCz4+Prc8dH7UqFFYvnw5Ro4cifHjx+Pq1auIjo4uVxhatmwZdu3ahT59+qB+/frIzs5WelK6d+8OABg6dCjWrl2Lhx9+GNOmTcP9998PvV6P8+fPY/fu3RgwYAAeffRRAIX/Xa9fvx6ffvopGjZsCCcnJwQHB6Ndu3Zo2rQpZsyYgfz8fHh6emLTpk348ccfK/QaWXh6emLGjBmYP38+nnzySQwePBjnzp1DRESE1RBEeHg4/ve//+HBBx/Es88+i1atWsFsNuPs2bPYsWMHpk+fXmbQa9asGRo1aoQXX3wRQgh4eXlhy5YtVsPOFRUcHIzvvvsOW7ZsQZ06deDu7q4KiiVVxfutT58+WLhwIYYPH44JEybg6tWreOutt6zC38288soriI2NRWhoKKZOnYqmTZsiOzsbp0+fxtatW7Fs2TKlV2PMmDFYtWoVEhMTb/qefvvttzF37lz06tULffr0sfo7Lt7zEhkZiXbt2qFv37548cUXkZ2djblz58LHx0d1hn8vLy+8/PLLmDNnDry8vBAWFob4+HhERETgySefRPPmzZWyY8eOxfvvv4/Bgwfj9ddfh6+vL5YsWYITJ05g586dN3093Nzc8N5772H06NH4559/8Nhjj8HX1xeXL1/Gr7/+isuXL2Pp0qWqx9zqs6Zp06aYMGEC3nvvPdjZ2aF37944ffo05syZg4CAACX4ubm54e2338aTTz6J7t27Y/z48fDz88PJkyfx66+/YvHixTete7Ume1Y3Va5PP/1UDB8+XDRu3Fi4ubkJvV4v6tevL0aNGiWOHTtmVf7jjz8W7dq1E05OTsLNzU20adPG6iiv//73v6JVq1bC0dFRGI1GMWDAAKujPkaPHi1cXV1LrZPZbBbR0dGiYcOGwsnJSdx3331i165dlXKU2c2UPMosJSVFzJ8/X3Tt2lXUrVtXODo6CldXV3HPPfeI+fPnq04Qd6ujzG52dJ4QRUdgHDhwQPTr10+4ubkJd3d3MWzYsFJPGLd582bx0EMPCQ8PD2EwGERgYKB47LHHxM6dO5UyZb3GpR3tsXPnTtGmTRthMBgEAOVIJEvZy5cvl1rvVatWibvvvls4OTmJ5s2bi08//bRcR2vt3btXPProoyIwMFAYDAbh7e0tOnfubHX0Xl5ennjrrbdE69atlfdcs2bNxMSJE8Wff/6plDt9+rQICwsT7u7uAoDq+f/44w8RFhYmPDw8RO3atcWUKVPE119/fVtHmQlR+P6MiooSAQEBwtHRUbRq1Ups2bKl1KPuMjIyxMsvvyyaNm2q/D0EBweLZ599ViQlJd30eY4dOyZ69Ogh3N3dhaenpxg8eLA4e/as1RGBZe2j0o6wO3z4sOjYsaNwcXFRHRVnKVvyiDiLyn6/rVy5UjRt2lQYDAbRsGFDERUVJVasWGFV38DAQNGnT59S63T58mUxdepUERQUJPR6vfDy8hJt27YVs2fPFhkZGUq5QYMGCWdnZ5GSklLqdiw6d+5807/hkn755RfRrVs34eLiIjw8PMQjjzwiTp48Weq23333XdGkSRPh6Ogo6tevL+bNmydyc3OtyiUlJYnHH39ceHl5CScnJ9GhQwcRGxt703oXFxcXJ/r06SO8vLyEXq8XdevWFX369BEbNmxQylTks6agoEC88cYbokmTJkKv1wsfHx8xcuRIce7cOavn3rp1q+jcubNwdXUVLi4uonnz5uKNN96wet6aRCfELU6eQUQVFhERgcjISFy+fLnShgWJqHDy8KhRo/Dmm2/Kroom8LOm8vCweyIiqhYSEhKQmZmJmTNnyq4K1UCcQ0RERNVCixYtkJaWJrsaVENxyIyIiIhsHofMiIiIyOYxEBEREZHNYyAiIiIim8dJ1eVkNptx4cIFuLu729SZO4mIiKozIQTS09Ph7+9vdb2+4hiIyunChQsVvkYWERERacO5c+duetFaBqJyslzf5ty5c1Ku6E5EREQVl5aWhoCAgFteQJqBqJwsw2QeHh4MRERERNXMraa7cFI1ERER2TwGIiIiIrJ5DERERERk8xiIiIiIyOYxEBEREZHNYyAiIiIim8dARERERDaPgYiIiIhsHgMRERER2TwGIiIiIrJ5DERERERk8xiIiIiIyObx4q6SXU7PQU5+ATxdHOFq4O4gIiKSgT1Ekj332WF0emM3tickya4KERGRzWIg0gghZNeAiIjIdjEQSabT6QAAzENERETyMBBJppNdASIiImIg0grBMTMiIiJpGIgkuzFixiEzIiIiiRiIJOOQGRERkXwMRFrBLiIiIiJpGIgkKzrKjImIiIhIFgYiyThkRkREJB8DkWTKpGp2EBEREUnDQKQRzENERETyMBBJd2MOERMRERGRNAxEkuk4iYiIiEg6BiKN4FFmRERE8jAQSWbpIOKQGRERkTwMRJJxyIyIiEg+BiKNYAcRERGRPAxEkunAExERERHJxkAkGYfMiIiI5GMg0gj2DxEREcnDQCQZL91BREQkHwORZDpe3pWIiEg6BiLZlB4idhERERHJwkCkEYxDRERE8jAQScYBMyIiIvkYiCTT6Xi1eyIiItkYiDSCeYiIiEgeBiLJii7uykhEREQkCwORZDxTNRERkXwMRERERGTzGIgkKxoyk1oNIiIim8ZAJJmOY2ZERETSMRBphOBxZkRERNIwEEnGITMiIiL5GIhk44gZERGRdAxEklmuds8OIiIiInkYiDSCQ2ZERETyMBBJxoPMiIiI5GMgkkyZVM1BMyIiImkYiDSCQ2ZERETyMBBJxiEzIiIi+RiIJNPxuHsiIiLpGIg0QnDMjIiISBqpgej7779Hv3794O/vD51Oh82bN6vWCyEQEREBf39/ODs7o0uXLkhISFCVycnJwZQpU+Dj4wNXV1f0798f58+fV5VJSUnBqFGjYDQaYTQaMWrUKFy7dq2KW1c+liEz5iEiIiJ5pAai69evo3Xr1li8eHGp66Ojo7Fw4UIsXrwY8fHxMJlM6NGjB9LT05Uy4eHh2LRpE9avX48ff/wRGRkZ6Nu3LwoKCpQyw4cPx+HDh7Ft2zZs27YNhw8fxqhRo6q8feXBOURERETyOch88t69e6N3796lrhNC4J133sHs2bMxcOBAAMCqVavg5+eHdevWYeLEiUhNTcWKFSuwevVqdO/eHQCwZs0aBAQEYOfOnejZsyeOHz+Obdu2Yd++fWjfvj0A4MMPP0RISAhOnDiBpk2b3pnGlolnqiYiIpJNs3OIEhMTkZSUhLCwMGWZwWBA586dsWfPHgDAgQMHkJeXpyrj7++Pli1bKmX27t0Lo9GohCEA6NChA4xGo1KmNDk5OUhLS1PdqhKHzIiIiOTRbCBKSkoCAPj5+amW+/n5KeuSkpLg6OgIT0/Pm5bx9fW12r6vr69SpjRRUVHKnCOj0YiAgIB/1Z6ycMiMiIhIPs0GIgtdicQghLBaVlLJMqWVv9V2Zs2ahdTUVOV27ty5Cta8fHimaiIiIvk0G4hMJhMAWPXiJCcnK71GJpMJubm5SElJuWmZS5cuWW3/8uXLVr1PxRkMBnh4eKhuVYlDZkRERPJoNhAFBQXBZDIhNjZWWZabm4u4uDiEhoYCANq2bQu9Xq8qc/HiRRw9elQpExISgtTUVPz8889Kmf379yM1NVUpIxOHzIiIiOSTepRZRkYGTp48qdxPTEzE4cOH4eXlhfr16yM8PBwLFixA48aN0bhxYyxYsAAuLi4YPnw4AMBoNGLcuHGYPn06vL294eXlhRkzZiA4OFg56uzuu+9Gr169MH78eCxfvhwAMGHCBPTt21cDR5gVnamaHURERETySA1Ev/zyCx566CHl/nPPPQcAGD16NGJiYvDCCy8gKysLkyZNQkpKCtq3b48dO3bA3d1decyiRYvg4OCAIUOGICsrC926dUNMTAzs7e2VMmvXrsXUqVOVo9H69+9f5rmPpOGYGRERkTQ6wWtGlEtaWhqMRiNSU1MrdT7R3C+O4uO9ZzC16114Lkx+jxUREVFNUt7vb83OIbIVRUeZERERkSwMRBrBfjoiIiJ5GIgks5wLiechIiIikoeBiIiIiGweA5FklvMQcciMiIhIHgYijWAeIiIikoeBSDIdeKpqIiIi2RiIJOOQGRERkXwMRBrBo8yIiIjkYSCSjANmRERE8jEQSabjqaqJiIikYyDSCOYhIiIieRiIJNPpOGhGREQkGwORZMqIGQ8zIyIikoaBSCOYh4iIiORhIJKNI2ZERETSMRBJZjlTNTuIiIiI5GEgkoxnqiYiIpKPgUgjeKZqIiIieRiIJOMUIiIiIvkYiCTjkBkREZF8DERERERk8xiIJNNx0IyIiEg6BiLJiobMOGZGREQkCwORRjAOERERycNAJBkHzIiIiORjIJLtxpgZR8yIiIjkYSCSTLnaPQfNiIiIpGEgIiIiIpvHQCQZT8xIREQkHwORZLzaPRERkXwMRERERGTzGIgk45AZERGRfAxEkhWdh4iJiIiISBYGIo1gDxEREZE8DESS6XiqaiIiIukYiCTT8UzVRERE0jEQaQTPVE1ERCQPAxERERHZPAYiyXjYPRERkXwMRJLxTNVERETyMRARERGRzWMgkoxDZkRERPIxEElmOQ0RjzIjIiKSh4GIiIiIbB4DkWS6oi4iIiIikoSBSDIeZUZERCQfAxERERHZPAYiyYqOMmMfERERkSwMRBrBOERERCQPA5FGsIOIiIhIHgYiyXTKYWZEREQkCwORZDzqnoiISD4GIsk4qZqIiEg+TQei/Px8vPzyywgKCoKzszMaNmyIV155BWazWSkjhEBERAT8/f3h7OyMLl26ICEhQbWdnJwcTJkyBT4+PnB1dUX//v1x/vz5O90cIiIi0ihNB6I33ngDy5Ytw+LFi3H8+HFER0fjzTffxHvvvaeUiY6OxsKFC7F48WLEx8fDZDKhR48eSE9PV8qEh4dj06ZNWL9+PX788UdkZGSgb9++KCgokNEsFQ6ZERERyecguwI3s3fvXgwYMAB9+vQBADRo0ACffPIJfvnlFwCFvUPvvPMOZs+ejYEDBwIAVq1aBT8/P6xbtw4TJ05EamoqVqxYgdWrV6N79+4AgDVr1iAgIAA7d+5Ez5495TTuBmVSNRMRERGRNJruIerUqRO+/fZb/PHHHwCAX3/9FT/++CMefvhhAEBiYiKSkpIQFhamPMZgMKBz587Ys2cPAODAgQPIy8tTlfH390fLli2VMqXJyclBWlqa6kZEREQ1k6Z7iGbOnInU1FQ0a9YM9vb2KCgowGuvvYZhw4YBAJKSkgAAfn5+qsf5+fnhzJkzShlHR0d4enpalbE8vjRRUVGIjIyszOaUqqiDiF1EREREsmi6h+jTTz/FmjVrsG7dOhw8eBCrVq3CW2+9hVWrVqnKlTyXjxDiluf3uVWZWbNmITU1VbmdO3fu9htyE8ocIuYhIiIiaTTdQ/T888/jxRdfxNChQwEAwcHBOHPmDKKiojB69GiYTCYAhb1AderUUR6XnJys9BqZTCbk5uYiJSVF1UuUnJyM0NDQMp/bYDDAYDBURbOIiIhIYzTdQ5SZmQk7O3UV7e3tlcPug4KCYDKZEBsbq6zPzc1FXFycEnbatm0LvV6vKnPx4kUcPXr0poHojrnRS8UeIiIiInk03UPUr18/vPbaa6hfvz5atGiBQ4cOYeHChRg7diyAwqGy8PBwLFiwAI0bN0bjxo2xYMECuLi4YPjw4QAAo9GIcePGYfr06fD29oaXlxdmzJiB4OBg5agzmYoOu2ciIiIikkXTgei9997DnDlzMGnSJCQnJ8Pf3x8TJ07E3LlzlTIvvPACsrKyMGnSJKSkpKB9+/bYsWMH3N3dlTKLFi2Cg4MDhgwZgqysLHTr1g0xMTGwt7eX0SyVojNVy60HERGRLdMJXjOiXNLS0mA0GpGamgoPD49K2+7a/Wcwe9NRhDX3wweP31dp2yUiIqLyf39reg6RLdDdGDRjKiUiIpKHgUgyDpkRERHJx0BERERENo+BSLKiU0Oyi4iIiEgWBiLJOGRGREQkHwMRERER2TwGIsl4lBkREZF8DESyKUNmjERERESyMBARERGRzWMgkqzoWmZEREQkCwORZDpe7Z6IiEg6BiLJdLcuQkRERFWMgUgj2EFEREQkDwORZDoeZUZERCQdA5FkOo6ZERERScdARERERDaPgUgy5UzVHDEjIiKShoFIMmUOEadVExERScNARERERDaPgUgjOGRGREQkDwORZDxTNRERkXwMRERERGTzGIgkK7q4K7uIiIiIZGEgkqzoTNVy60FERGTLGIgk0/HyrkRERNIxEGkEO4iIiIjkYSCSTFc0iYiIiIgkYSCSjANmRERE8jEQaQSPMiMiIpKHgUgyHmVGREQkHwORdDfOVC25FkRERLaMgYiIiIhsHgORZEVDZuwjIiIikoWBSDIedU9ERCQfA5FkOh0PvCciIpKNgUgjOGJGREQkDwORZBwyIyIiko+BSDKOmBEREclXoUAUHR2NrKws5f7333+PnJwc5X56ejomTZpUebWzJRwzIyIikqZCgWjWrFlIT09X7vft2xd///23cj8zMxPLly+vvNrZAOWwe7nVICIismkVCkQlz5XDc+f8ezpe3pWIiEg6ziHSCGZLIiIieRiIZFOGzJiIiIiIZHGo6AP++9//ws3NDQCQn5+PmJgY+Pj4AIBqfhGVDwfMiIiI5KtQIKpfvz4+/PBD5b7JZMLq1autylDFcciMiIhIngoFotOnT1dRNWyX5dIdDERERETycA6RZDxTNRERkXwVCkT79+/HN998o1r28ccfIygoCL6+vpgwYYLqRI10azxTNRERkXwVCkQRERH47bfflPtHjhzBuHHj0L17d7z44ovYsmULoqKiKr2SNZnlPEQ8pxMREZE8FQpEhw8fRrdu3ZT769evR/v27fHhhx/iueeew//93//hs88+q/RK1mTKmaqZh4iIiKSpUCBKSUmBn5+fcj8uLg69evVS7rdr1w7nzp2rvNrZAB3PQ0RERCRdhQKRn58fEhMTAQC5ubk4ePAgQkJClPXp6enQ6/WVW8Mazu5GIjIzDxEREUlToUDUq1cvvPjii/jhhx8wa9YsuLi44IEHHlDW//bbb2jUqFGlV7Ims8ypNnPMjIiISJoKnYdo/vz5GDhwIDp37gw3NzfExMTA0dFRWb9y5UqEhYVVeiVrMjs7Xu6eiIhItgr1ENWuXRs//PADUlJSkJKSgoEDB6rWb9iwAREREZVZP/z9998YOXIkvL294eLignvuuQcHDhxQ1gshEBERAX9/fzg7O6NLly5ISEhQbSMnJwdTpkyBj48PXF1d0b9/f5w/f75S63m7LHmIPURERETyVKiHaOzYseUqt3LlytuqTEkpKSno2LEjHnroIXzzzTfw9fXFX3/9hVq1ailloqOjsXDhQsTExKBJkyaYP38+evTogRMnTsDd3R0AEB4eji1btmD9+vXw9vbG9OnT0bdvXxw4cAD29vaVUtfbxzlEREREslUoEMXExCAwMBBt2rS5I+fNeeONNxAQEICPPvpIWdagQQPldyEE3nnnHcyePVvprVq1ahX8/Pywbt06TJw4EampqVixYgVWr16N7t27AwDWrFmDgIAA7Ny5Ez179qzydtyMHY8yIyIikq5Cgeipp57C+vXrcerUKYwdOxYjR46El5dXVdUNX375JXr27InBgwcjLi4OdevWxaRJkzB+/HgAQGJiIpKSklTzlgwGAzp37ow9e/Zg4sSJOHDgAPLy8lRl/P390bJlS+zZs6fMQJSTk6M663ZaWlqVtFE5ysxcJZsnIiKicqjQHKIlS5bg4sWLmDlzJrZs2YKAgAAMGTIE27dvr5Ieo1OnTmHp0qVo3Lgxtm/fjqeeegpTp07Fxx9/DABISkoCANW5kSz3LeuSkpLg6OgIT0/PMsuUJioqCkajUbkFBARUZtMURSdmZA8RERGRLBW+uKvBYMCwYcMQGxuLY8eOoUWLFpg0aRICAwORkZFRqZUzm8249957sWDBArRp0wYTJ07E+PHjsXTpUlU5XYkLggkhrJaVdKsys2bNQmpqqnKrqhNOWnqIGIeIiIjk+VdXu9fpdNDpdBBCwFwFYz516tRB8+bNVcvuvvtunD17FgBgMpkAwKqnJzk5Wek1MplMyM3NRUpKSpllSmMwGODh4aG6VQUdjzIjIiKSrsKBKCcnB5988gl69OiBpk2b4siRI1i8eDHOnj0LNze3Sq1cx44dceLECdWyP/74A4GBgQCAoKAgmEwmxMbGKutzc3MRFxeH0NBQAEDbtm2h1+tVZS5evIijR48qZWTS8SgzIiIi6So0qXrSpElYv3496tevjyeeeEI5jL2qPPvsswgNDcWCBQswZMgQ/Pzzz/jggw/wwQcfACjsoQoPD8eCBQvQuHFjNG7cGAsWLICLiwuGDx8OADAajRg3bhymT58Ob29veHl5YcaMGQgODlaOOpPJ7kYkZQcRERGRPBUKRMuWLUP9+vURFBSEuLg4xMXFlVpu48aNlVK5du3aYdOmTZg1axZeeeUVBAUF4Z133sGIESOUMi+88AKysrIwadIkpKSkoH379tixY4dyDiIAWLRoERwcHDBkyBBkZWWhW7duiImJ0cA5iIrNIWIiIiIikkYnKvBNPGbMmFtOVgagOm9QTZGWlgaj0YjU1NRKnU/056V09Fj0PTxd9Dg0l5c9ISIiqkzl/f6u8IkZqXLpeJQZERGRdP/qKDP695RrmXFWNRERkTQMRJIpPUTMQ0RERNIwEElWdC0zIiIikoWBSDLlWmbsIiIiIpKGgUgjGIiIiIjkYSCSzM6OZ6omIiKSjYFIMsscIk4iIiIikoeBSDLOISIiIpKPgUgySwcRAxEREZE8DESS8UzVRERE8jEQSaach0jwAq9ERESyMBBJVvxiucxDREREcjAQSWZXlIc4bEZERCQJA5FkxXuIOLGaiIhIDgYiyYrlIQYiIiIiSRiIJLPjHCIiIiLpGIgkU80hYiAiIiKSgoFIMh04h4iIiEg2BiLJdDzKjIiISDoGIsnseJQZERGRdAxEkql6iMzy6kFERGTLGIgkUx1lxkEzIiIiKRiIJLNTnYdIXj2IiIhsGQORZDxTNRERkXwMRBqgK3bFeyIiIrrzGIg0wDKPSDARERERScFApAGWQTPOISIiIpKDgUgDlB4iHmVGREQkBQORBljmELGHiIiISA4GIg1QAhETERERkRQMRBpQ/OSMREREdOcxEGmAJRDxPERERERyMBBpAI8yIyIikouBSAOKJlUzEREREcnAQKQBdnaWEzNKrggREZGNYiDSAJ6pmoiISC4GIg3gHCIiIiK5GIg0QMczVRMREUnFQKQBdsqJGeXWg4iIyFYxEGkAjzIjIiKSi4FIA3imaiIiIrkYiDSAZ6omIiKSi4FIQ3iUGRERkRwMRBpgd2Mv8DxEREREcjAQaUDRkJnkihAREdkoBiINsEypZg8RERGRHAxEGqBcukNyPYiIiGwVA5EGKOch4pgZERGRFAxEGqDjHCIiIiKpGIg0wHLpDl7LjIiISA4GIg1QjjLjtcyIiIikYCDSAJ6pmoiISC4GIg1wsC8MRAWcRERERCRFtQpEUVFR0Ol0CA8PV5YJIRAREQF/f384OzujS5cuSEhIUD0uJycHU6ZMgY+PD1xdXdG/f3+cP3/+Dte+bPY3JhHlMxARERFJUW0CUXx8PD744AO0atVKtTw6OhoLFy7E4sWLER8fD5PJhB49eiA9PV0pEx4ejk2bNmH9+vX48ccfkZGRgb59+6KgoOBON6NUDnaWHiJOIiIiIpKhWgSijIwMjBgxAh9++CE8PT2V5UIIvPPOO5g9ezYGDhyIli1bYtWqVcjMzMS6desAAKmpqVixYgXefvttdO/eHW3atMGaNWtw5MgR7Ny5U1aTVCw9RHkF7CEiIiKSoVoEosmTJ6NPnz7o3r27anliYiKSkpIQFhamLDMYDOjcuTP27NkDADhw4ADy8vJUZfz9/dGyZUulTGlycnKQlpamulUVhxtXd+UcIiIiIjkcZFfgVtavX4+DBw8iPj7eal1SUhIAwM/PT7Xcz88PZ86cUco4OjqqepYsZSyPL01UVBQiIyP/bfXLhXOIiIiI5NJ0D9G5c+cwbdo0rFmzBk5OTmWWs5zp2UIIYbWspFuVmTVrFlJTU5XbuXPnKlb5CtDbcw4RERGRTJoORAcOHEBycjLatm0LBwcHODg4IC4uDv/3f/8HBwcHpWeoZE9PcnKyss5kMiE3NxcpKSlllimNwWCAh4eH6lZV2ENEREQkl6YDUbdu3XDkyBEcPnxYud13330YMWIEDh8+jIYNG8JkMiE2NlZ5TG5uLuLi4hAaGgoAaNu2LfR6varMxYsXcfToUaWMbJxDREREJJem5xC5u7ujZcuWqmWurq7w9vZWloeHh2PBggVo3LgxGjdujAULFsDFxQXDhw8HABiNRowbNw7Tp0+Ht7c3vLy8MGPGDAQHB1tN0pZF6SHiUWZERERSaDoQlccLL7yArKwsTJo0CSkpKWjfvj127NgBd3d3pcyiRYvg4OCAIUOGICsrC926dUNMTAzs7e0l1rxI0XmIGIiIiIhk0AnBC2iVR1paGoxGI1JTUyt9PtHzG37FhgPnMbNXMzzdpVGlbpuIiMiWlff7W9NziGyFA48yIyIikoqBSAN4lBkREZFcDEQawKPMiIiI5GIg0gBey4yIiEguBiIN4NXuiYiI5GIg0gDOISIiIpKLgUgDHOw5h4iIiEgmBiINcGAPERERkVQMRBpgGTIr4KRqIiIiKRiINIA9RERERHIxEGmAPY8yIyIikoqBSAPYQ0RERCQXA5EG2PMoMyIiIqkYiDSAPURERERyMRBpgHJixgLOISIiIpKBgUgD9Pa8lhkREZFMDEQa4Ky3BwBk5xVIrgkREZFtYiDSAIMlEOUzEBEREcnAQKQBlh6irFwGIiIiIhkYiDTASRky46RqIiIiGRiINMBJX7gbOIeIiIhIDgYiDeCkaiIiIrkYiDTAMmSWlVcAIXjoPRER0Z3GQKQBlkBkFjwXERERkQwMRBpgmUMEFPYSERER0Z3FQKQBjvZ2uHH1DuQwEBEREd1xDEQaoNPpVPOIiIiI6M5iINIIF8fCQJSRky+5JkRERLaHgUgjPF0cAQDXMvMk14SIiMj2MBBphJdrYSC6ej1Xck2IiIhsDwORRni7FQaifzJyJNeEiIjI9jAQaYSlh+gf9hARERHdcQxEGuHlagAAXGEgIiIiuuMYiDSibi0nAMC5fzIl14SIiMj2MBBpxF2+bgCAPy9lSK4JERGR7WEg0oi7arsDAJLSsnEtk8NmREREdxIDkUYYXfRoVNsVAPDTyauSa0NERGRbGIg0pEtTXwDA7hPJkmtCRERkWxiINKRHcz8AwNYjF5GWzTNWExER3SkMRBrSPsgLjX3dkJlbgM9/OS+7OkRERDaDgUhDdDodRoc2AAB88P0pZPPK90RERHcEA5HGDL6vHvyNTkhKy8a6/WdlV4eIiMgmMBBpjMHBHlO6NQYALPnuJK7n5EuuERERUc3HQKRBj7Wth0BvF1zJyMWyuL9kV4eIiKjGYyDSIL29HWb1bgYAWP79KV7Og4iIqIoxEGlUzxYmhDbyRm6+GQu2HpddHSIiohqNgUijdDod5vVrAXs7Hb45moTveLJGIiKiKsNApGFNTe4YHdIAADB701FkcII1ERFRlWAg0rgZPZsgwMsZf1/Lwhvf/C67OkRERDUSA5HGuTg64PWBrQAAq/edwd6/eOFXIiKiysZAVA10vMsHw+6vDwB49tPD+Od6ruQaERER1SwMRNXEy33uRsParkhKy8aMDb/CbBayq0RERFRjMBBVE64GB7w//F44Othh1+/J+PCHU7KrREREVGMwEFUjd9fxwNy+zQEAb2z7Hbt/56H4RERElUHTgSgqKgrt2rWDu7s7fH198cgjj+DEiROqMkIIREREwN/fH87OzujSpQsSEhJUZXJycjBlyhT4+PjA1dUV/fv3x/nz5+9kUyrNiPb18Z/7AmAWwJRPDuFEUrrsKhEREVV7mg5EcXFxmDx5Mvbt24fY2Fjk5+cjLCwM169fV8pER0dj4cKFWLx4MeLj42EymdCjRw+kpxcFhfDwcGzatAnr16/Hjz/+iIyMDPTt2xcFBQUymvWv6HQ6vPpIS7QP8kJGTj7GxsQjOS1bdrWIiIiqNZ0QotrMzr18+TJ8fX0RFxeHBx98EEII+Pv7Izw8HDNnzgRQ2Bvk5+eHN954AxMnTkRqaipq166N1atX4z//+Q8A4MKFCwgICMDWrVvRs2fPcj13WloajEYjUlNT4eHhUWVtLK+U67l4dMlPOH01E4193fDpxBB4uTrKrhYREZGmlPf7W9M9RCWlpqYCALy8vAAAiYmJSEpKQlhYmFLGYDCgc+fO2LNnDwDgwIEDyMvLU5Xx9/dHy5YtlTKlycnJQVpamuqmJZ6ujlg9rj1MHk74MzkDj6/cj7TsPNnVIiIiqpaqTSASQuC5555Dp06d0LJlSwBAUlISAMDPz09V1s/PT1mXlJQER0dHeHp6llmmNFFRUTAajcotICCgMptTKQK8XLDmyfbwdnXE0b/TMOq/+3mOIiIiottQbQLRM888g99++w2ffPKJ1TqdTqe6L4SwWlbSrcrMmjULqampyu3cuXO3V/EqdpevGz4edz88XfT49Xwqhizfi4upWbKrRUREVK1Ui0A0ZcoUfPnll9i9ezfq1aunLDeZTABg1dOTnJys9BqZTCbk5uYiJSWlzDKlMRgM8PDwUN20qoW/ERueCkEdoxNOJmfgsaV78eclHn1GRERUXpoOREIIPPPMM9i4cSN27dqFoKAg1fqgoCCYTCbExsYqy3JzcxEXF4fQ0FAAQNu2baHX61VlLl68iKNHjyplaoK7fN3x+dOhaOjjir+vZeGR939C7LFLsqtFRERULWg6EE2ePBlr1qzBunXr4O7ujqSkJCQlJSErq3BISKfTITw8HAsWLMCmTZtw9OhRjBkzBi4uLhg+fDgAwGg0Yty4cZg+fTq+/fZbHDp0CCNHjkRwcDC6d+8us3mVrm4tZ3z+dCg6NPTC9dwCjP/4F/zft3/yMh9ERES3oOnD7sua4/PRRx9hzJgxAAp7kSIjI7F8+XKkpKSgffv2eP/995WJ1wCQnZ2N559/HuvWrUNWVha6deuGJUuWVGiitNYOu7+ZvAIzXv3qGD7eewYAENrIGwuH3AOT0UlyzYiIiO6s8n5/azoQaUl1CkQWn8Wfw7wvE5CVV4BaLnq8PjAYvVrWkV0tIiKiO6ZGnoeIKmZIuwB8NbUTWtb1wLXMPDy15iCeWn0ASak8szUREVFxDEQ1XKPabtj4dEdM6tIIDnY6bEtIQveFcfjop0TkFZhlV4+IiEgTOGRWTtVxyKyk35PS8NLGIzh49hoAoKGPK17o1RQ9W5hued4mIiKi6ohziCpZTQhEAGA2C3wSfxYLd/yBqzfOan1v/Vp4rkdTdLzLm8GIiIhqFAaiSlZTApFFenYePvz+FD78IRFZeQUAgNb1jHi6SyOENTfBzo7BiIiIqj8GokpW0wKRRXJaNpZ89xfWx59Fdl7hnKKGtV0xsn0gBrWtB6OzXnINiYiIbh8DUSWrqYHI4mpGDmL2nEbMntNIz84HADjp7dCvlT+Gta+PNgG1OJxGRETVDgNRJavpgcgiIycfmw79jbX7zuD3pKLrodX3csGAe/zRv7U/Gvu5S6whERFR+TEQVTJbCUQWQggcPJuCtfvO4pujSco8IwBoZnJHWHM/dL3bD63qGjnfiIiINIuBqJLZWiAqLjM3H7HHLmHLrxfw3YnLyC92bTQfNwO6NquNLk190aGhN7xcHSXWlIiISI2BqJLZciAqLuV6Lr79PRm7fr+E7/+4goycfNX6ZiZ3dGjojQ4NvXB/EAMSERHJxUBUyRiIrOXmmxF/+h98ezwZP528ghOX0q3KBHq7oHW9WmgdUAv3BBjRwt8IJ729hNoSEZEtYiCqZAxEt3Y1Iwf7E//BvlNXsfevq/gzOcOqjL2dDnfVdkNTkzuamtzRzOSOJn7uqOfpzKPYiIio0jEQVTIGoopLzczDb39fw6/nruHwuVQcPncNVzJySi3rZnBAYz83BPm4IsjbFYHKTxd4OPFcSEREdHsYiCoZA9G/J4TAxdRsHL+YhhOX0nEiqfD21+UM5BWU/Tb0dnVEoLcL6nm6oE4tJ9St5Yw6Rmf413KCv9EZtVz07F0iIqJSMRBVMgaiqpNXYEbilev441I6Tl+5jtNXM5WfZfUoFeest0edWk4weTjBx82A2u6GYj8dUdvdgNpuBni5OsLB3u4OtIiIiLSivN/fDnewTkSl0tvboYlf4VyiktKz83DmaibOXM3EhWtZ+PtaFi6mZuHCtWxcTM3ClYxcZOUV4NTl6zh1+fpNn0enA7xcHOHp6ohaznrUctHD6OwIT5cbv7sULfd0cYTRWQ8PZz3cDA6w57mWiIhqNAYi0jR3Jz1a1jWiZV1jqeuz8wqQlJqNC9eycCk9G1fSc3ElIweX03Nw+cbPKxm5+Od6DswCuHo9F1ev51a4Hi6O9nAzOMDNyQHuN366Oqrvuxn0N37aw1nvABdHezg72sNZbw8nvX3hfX3hMoODHYf5iIg0hIGIqjUnvT0a+LiigY/rTcsVmAX+uZ6Ly+k5uJaVi9TMPKRk5im/X8vMQ0pmLq5l5RXez8pFSmYecvMLL3ibmVuAzNwCJKffegivPHS6wqE+S1hydiwMTE7KMjs4OtjD0d4OBr1d4U+HwpvjjZvBwb7w92Jlii83KOUKfzrY2UFvr4ODvR0c7HTQ29ux54uI6AYGIrIJ9na6wrlE7oYKPS4nvwAZ2fnIyMlH+o2f13PU99Xr83A9pwBZeYUBKjuvAFm5BcjMzUd2nhm5BYUBS4iikCWTnQ5wsLeD3q4wKOntdXCws4ODfWFgclAtL1ymty9cXzxgFT6+KGzZ6QrL29vpYGeng72u8HfVTVe4zqFYGeV3O8Dezu7G41C4PfvC7RZ/fPHtWcpYtmtZr9MVPr7wBuhu/FSW2RX9XlQWyn325BHZBgYiopswONjD4GYPb7eKBamy5BeYkZ1vLgxIuWZk5VnCU/6N8FS4LiffjNx8s/Izt6AAOTcCleVn4fqCUsoWLs8tsbz4JVcszKLwBJuFg4hyw5lWFQ9JljBlXzxA2ZUdtiyPVQczS9gqXrbEY+1uLIMllBX/XQddsXpZfgcsz1H+x6HY+pKPAyz1Kl5Gp7wmqseV2F7JxxV/7pKPK769mz+u6DWC5fmKPRbFykF5TW6sU34v9pzFy5TYRtG+L6XsjbqVXI4Syy2PV9WzInUqpU3Qlb5cpyu7TqWWLeO1Q7HlVm296etc+vOU1aaSz1f8vo+bQdrJexmIiO4gB3s7uNnbwc1w5//0hBDINwvkFwjkmc3ILxDILygMUPkFAvlmM/IKrNfnmW/8vFEmv0Agr8B8Y1tFyy2PLTCbUSAECswo/N0MmEVhmQIzYDYX1sMsBArMxW7F7pvFjW2Vskz1uBLbMN9oY4FZwGwWMIvC5xY3fhbeKvq6ofB5Cu9VwZ4hIouPx96PB5vUlvLcDERENkKn00Fvr4PeHnCGbV8+RYiisGQJTJZAZRalr7esM5utA5YQheHMbC49gN10e0IUPt5c9Lv5Rn3EjboW1rlo25blyk9l2Y0yNx4gcKO+N9apyxc9zrLd4q9NyTIlH2e+8TuKrbd6nCj+3EXroawvqm/ZjyssA9VzF70mlohq2W7R70X1Kvq9aDmKrSvaVrHtKmWtnwsly5bYntVzlbq8tOeyrlNZdbC8RkX1uVkdiu9fZfO3Lqsqf/M6WPZbUcnS61V8fcn2AUW9kDIwEBGRzdHpdLDXAfaQ9+FLRNrCs9QRERGRzWMgIiIiIpvHQEREREQ2j4GIiIiIbB4DEREREdk8BiIiIiKyeQxEREREZPMYiIiIiMjmMRARERGRzWMgIiIiIpvHQEREREQ2j4GIiIiIbB4DEREREdk8BiIiIiKyeQ6yK1BdCCEAAGlpaZJrQkREROVl+d62fI+XhYGonNLT0wEAAQEBkmtCREREFZWeng6j0Vjmep24VWQiAIDZbMaFCxfg7u4OnU5XadtNS0tDQEAAzp07Bw8Pj0rbrpbU9DayfdVfTW9jTW8fUPPbyPbdPiEE0tPT4e/vDzu7smcKsYeonOzs7FCvXr0q276Hh0eNfJMXV9PbyPZVfzW9jTW9fUDNbyPbd3tu1jNkwUnVREREZPMYiIiIiMjmMRBJZjAYMG/ePBgMBtlVqTI1vY1sX/VX09tY09sH1Pw2sn1Vj5OqiYiIyOaxh4iIiIhsHgMRERER2TwGIiIiIrJ5DERERERk8xiIJFuyZAmCgoLg5OSEtm3b4ocffpBdpVuKiopCu3bt4O7uDl9fXzzyyCM4ceKEqsyYMWOg0+lUtw4dOqjK5OTkYMqUKfDx8YGrqyv69++P8+fP38mmlCkiIsKq/iaTSVkvhEBERAT8/f3h7OyMLl26ICEhQbUNLbevQYMGVu3T6XSYPHkygOq5/77//nv069cP/v7+0Ol02Lx5s2p9Ze2zlJQUjBo1CkajEUajEaNGjcK1a9equHU3b19eXh5mzpyJ4OBguLq6wt/fH48//jguXLig2kaXLl2s9uvQoUM13z6g8t6TstoH3LqNpf1N6nQ6vPnmm0oZLe/D8nw3aPnvkIFIok8//RTh4eGYPXs2Dh06hAceeAC9e/fG2bNnZVftpuLi4jB58mTs27cPsbGxyM/PR1hYGK5fv64q16tXL1y8eFG5bd26VbU+PDwcmzZtwvr16/Hjjz8iIyMDffv2RUFBwZ1sTplatGihqv+RI0eUddHR0Vi4cCEWL16M+Ph4mEwm9OjRQ7nmHaDt9sXHx6vaFhsbCwAYPHiwUqa67b/r16+jdevWWLx4canrK2ufDR8+HIcPH8a2bduwbds2HD58GKNGjZLavszMTBw8eBBz5szBwYMHsXHjRvzxxx/o37+/Vdnx48er9uvy5ctV67XYPovKeE/Kah9w6zYWb9vFixexcuVK6HQ6DBo0SFVOq/uwPN8Nmv47FCTN/fffL5566inVsmbNmokXX3xRUo1uT3JysgAg4uLilGWjR48WAwYMKPMx165dE3q9Xqxfv15Z9vfffws7Ozuxbdu2qqxuucybN0+0bt261HVms1mYTCbx+uuvK8uys7OF0WgUy5YtE0Jov30lTZs2TTRq1EiYzWYhRPXffwDEpk2blPuVtc+OHTsmAIh9+/YpZfbu3SsAiN9//72KW1WkZPtK8/PPPwsA4syZM8qyzp07i2nTppX5GC23rzLek1ppnxDl24cDBgwQXbt2VS2rLvtQCOvvBq3/HbKHSJLc3FwcOHAAYWFhquVhYWHYs2ePpFrdntTUVACAl5eXavl3330HX19fNGnSBOPHj0dycrKy7sCBA8jLy1O139/fHy1bttRM+//880/4+/sjKCgIQ4cOxalTpwAAiYmJSEpKUtXdYDCgc+fOSt2rQ/sscnNzsWbNGowdO1Z14eLqvv+Kq6x9tnfvXhiNRrRv314p06FDBxiNRs21OzU1FTqdDrVq1VItX7t2LXx8fNCiRQvMmDFD9Z+51tv3b9+TWm9fcZcuXcLXX3+NcePGWa2rLvuw5HeD1v8OeXFXSa5cuYKCggL4+fmplvv5+SEpKUlSrSpOCIHnnnsOnTp1QsuWLZXlvXv3xuDBgxEYGIjExETMmTMHXbt2xYEDB2AwGJCUlARHR0d4enqqtqeV9rdv3x4ff/wxmjRpgkuXLmH+/PkIDQ1FQkKCUr/S9t2ZM2cAQPPtK27z5s24du0axowZoyyr7vuvpMraZ0lJSfD19bXavq+vr6banZ2djRdffBHDhw9XXShzxIgRCAoKgslkwtGjRzFr1iz8+uuvypCplttXGe9JLbevpFWrVsHd3R0DBw5ULa8u+7C07wat/x0yEElW/D9yoPBNVHKZlj3zzDP47bff8OOPP6qW/+c//1F+b9myJe677z4EBgbi66+/tvoDL04r7e/du7fye3BwMEJCQtCoUSOsWrVKmch5O/tOK+0rbsWKFejduzf8/f2VZdV9/5WlMvZZaeW11O68vDwMHToUZrMZS5YsUa0bP3688nvLli3RuHFj3HfffTh48CDuvfdeANptX2W9J7XavpJWrlyJESNGwMnJSbW8uuzDsr4bAO3+HXLITBIfHx/Y29tbpdnk5GSr9KxVU6ZMwZdffondu3ejXr16Ny1bp04dBAYG4s8//wQAmEwm5ObmIiUlRVVOq+13dXVFcHAw/vzzT+Vos5vtu+rSvjNnzmDnzp148sknb1quuu+/ytpnJpMJly5dstr+5cuXNdHuvLw8DBkyBImJiYiNjVX1DpXm3nvvhV6vV+1XLbevuNt5T1aX9v3www84ceLELf8uAW3uw7K+G7T+d8hAJImjoyPatm2rdHNaxMbGIjQ0VFKtykcIgWeeeQYbN27Erl27EBQUdMvHXL16FefOnUOdOnUAAG3btoVer1e1/+LFizh69Kgm25+Tk4Pjx4+jTp06Snd18brn5uYiLi5OqXt1ad9HH30EX19f9OnT56blqvv+q6x9FhISgtTUVPz8889Kmf379yM1NVV6uy1h6M8//8TOnTvh7e19y8ckJCQgLy9P2a9abl9Jt/OerC7tW7FiBdq2bYvWrVvfsqyW9uGtvhs0/3d429Ox6V9bv3690Ov1YsWKFeLYsWMiPDxcuLq6itOnT8uu2k09/fTTwmg0iu+++05cvHhRuWVmZgohhEhPTxfTp08Xe/bsEYmJiWL37t0iJCRE1K1bV6SlpSnbeeqpp0S9evXEzp07xcGDB0XXrl1F69atRX5+vqymKaZPny6+++47cerUKbFv3z7Rt29f4e7uruyb119/XRiNRrFx40Zx5MgRMWzYMFGnTp1q0z4hhCgoKBD169cXM2fOVC2vrvsvPT1dHDp0SBw6dEgAEAsXLhSHDh1SjrKqrH3Wq1cv0apVK7F3716xd+9eERwcLPr27Su1fXl5eaJ///6iXr164vDhw6q/y5ycHCGEECdPnhSRkZEiPj5eJCYmiq+//lo0a9ZMtGnTRvPtq8z3pKz23aqNFqmpqcLFxUUsXbrU6vFa34e3+m4QQtt/hwxEkr3//vsiMDBQODo6invvvVd16LpWASj19tFHHwkhhMjMzBRhYWGidu3aQq/Xi/r164vRo0eLs2fPqraTlZUlnnnmGeHl5SWcnZ1F3759rcrI8p///EfUqVNH6PV64e/vLwYOHCgSEhKU9WazWcybN0+YTCZhMBjEgw8+KI4cOaLahpbbJ4QQ27dvFwDEiRMnVMur6/7bvXt3qe/L0aNHCyEqb59dvXpVjBgxQri7uwt3d3cxYsQIkZKSIrV9iYmJZf5d7t69WwghxNmzZ8WDDz4ovLy8hKOjo2jUqJGYOnWquHr1qubbV5nvSVntu1UbLZYvXy6cnZ3FtWvXrB6v9X14q+8GIbT9d6i70QgiIiIim8U5RERERGTzGIiIiIjI5jEQERERkc1jICIiIiKbx0BERERENo+BiIiIiGweAxERERHZPAYiIqJ/SafTYfPmzbKrQUT/AgMREVVrY8aMgU6ns7r16tXrjtXh4sWL6N279x17PiKqfA6yK0BE9G/16tULH330kWqZwWC4Y89vuYo3EVVf7CEiomrPYDDAZDKpbp6engAKh7OWLl2K3r17w9nZGUFBQdiwYYPq8UeOHEHXrl3h7OwMb29vTJgwARkZGaoyK1euRIsWLWAwGFCnTh0888wzyjoOmRFVfwxERFTjzZkzB4MGDcKvv/6KkSNHYtiwYTh+/DgAIDMzE7169YKnpyfi4+OxYcMG7Ny5UxV4li5dismTJ2PChAk4cuQIvvzyS9x1112ymkNEVYAXdyWiam3MmDFYs2YNnJycVMtnzpyJOXPmQKfT4amnnsLSpUuVdR06dMC9996LJUuW4MMPP8TMmTNx7tw5uLq6AgC2bt2Kfv364cKFC/Dz80PdunXxxBNPYP78+aXWQafTYdOmTXjkkUeqrJ1EVLU4h4iIqr2HHnpIFXgAwMvLS/k9JCREtS4kJASHDx8GABw/fhytW7dWwhAAdOzYEWazGSdOnIBOp8OFCxfQrVu3qmsAEUnHQERE1Z6rq2uFh7B0Oh0AQAih/F5aGWdn539dPyLSPs4hIqIab9++fVb3mzVrBgBo3rw5Dh8+jOvXryvrf/rpJ9jZ2aFJkyZwd3dHgwYN8O23397ROhPRncUeIiKq9nJycpCUlKRa5uDgAB8fHwDAhg0bcN9996FTp05Yu3Ytfv75Z6xYsQIAMGLECMybNw+jR49GREQELl++jClTpmDUqFHw8/MDAEREROCpp56Cr68vevfujfT0dPz000+YMmXKnW0oEVUZBiIiqva2bduGOnXqqJY1bdoUv//+OwAgMjIS69evx6RJk2AymbB27Vo0b94cAODi4oLt27dj2rRpaNeuHVxcXDBo0CAsXLhQ2dbo0aORnZ2NRYsWYcaMGfDx8cFjjz125xpIRFWOR5kRUY3GI8CIqDw4h4iIiIhsHgMRERER2TzOISKiGo2zAoioPNhDRERERDaPgYiIiIhsHgMRERER2TwGIiIiIrJ5DERERERk8xiIiIiIyOYxEBEREZHNYyAiIiIim8dARERERDbv/wGSP3UWLmAr4QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mses_train)\n",
    "plt.title(f'Scorul MSE pentru setul de antrenare, {n_epochs} epoci')\n",
    "plt.xlabel('Epoci')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59b5a3",
   "metadata": {},
   "source": [
    "## Model de regresie liniara determinat prin metoda ecuatiilor normale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd99f48",
   "metadata": {},
   "source": [
    "Pentru regresia liniara exista o metoda analitica de determinare a coeficientilor $\\boldsymbol{\\theta}$ bazata pe metoda ecuatiilor normale. Pentru un set de date $X_{train}$ (nu neaparat cu coloanele scalate in intervalul [0, 1]) si un set de valori de iesire (ground truth) asociate $y_{train}$, vectorul de coeficienti se determina cu:\n",
    "$$\n",
    "\\boldsymbol{\\theta} = \\left(X_{train}^t \\cdot X_{train}\\right)^{-1}\\cdot X_{train} \\cdot y\n",
    "$$\n",
    "\n",
    "Expresia $X_{train} ^ \\dagger = \\left(X_{train}^t \\cdot X_{train}\\right)^{-1}\\cdot X_{train}$ se numeste pseudo inversa Moore-Penrose a lui $X_{train}$. Pentru calculul ei se foloseste functia [numpy.linalg.pinv](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75757eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE pe set de antrenare, cu metoda gradient descent: 112.06478435353691\n",
      "MSE pe set de antrenare, cu metoda ecuatiilor normale: 110.7779902085299\n",
      "MSE pe set de testare, cu metoda gradient descent: 112.06478435353691\n",
      "MSE pe set de testare, cu metoda ecuatiilor normale: 102.73965133120751\n"
     ]
    }
   ],
   "source": [
    "# calculeaza vectorul theta_norm folosind functia np.linalg.pinv\n",
    "theta_norm:np.ndarray = np.dot(np.linalg.pinv(X_train), y_train)\n",
    "# evalueaza modelul construit cu vectorul theta_norm pe setul de antrenare nescalat\n",
    "mse_norm_train:float = eval_model(X_train, y_train, theta_norm)\n",
    "# evalueaza modelul construit cu vectorul theta_norm pe setul de testare nescalat\n",
    "mse_norm_test:float = eval_model(X_test, y_test, theta_norm)\n",
    "# evalueaza modelul construit cu vectorul theta (folosidn metoda gradient descent) pe setul de antrenare scalat\n",
    "mse_grad_test:float = eval_model(X_train_scaled, y_train, theta)\n",
    "\n",
    "print(f'MSE pe set de antrenare, cu metoda gradient descent: {mses_train[-1]}')\n",
    "print(f'MSE pe set de antrenare, cu metoda ecuatiilor normale: {mse_norm_train}')\n",
    "\n",
    "print(f'MSE pe set de testare, cu metoda gradient descent: {mse_grad_test}')\n",
    "print(f'MSE pe set de testare, cu metoda ecuatiilor normale: {mse_norm_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f11f5c",
   "metadata": {},
   "source": [
    "## Intrebari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de580be4",
   "metadata": {},
   "source": [
    "Intrebarile din aceasta sectiune sunt optionale, dar tratarea lor este utila pentru intelegerea mai buna a materialului.\n",
    "\n",
    "1. Vectorii `min_cols` si `max_cols` determinati pe setul de antrenare se aplica pe setul de testare. Puteti explica de ce dupa acest pas, pe setul de testare valorile minime si maxime de coloane nu sunt 0, respectiv 1?\n",
    "Scalarea se face relativ la valorile de pe setul de antrenare. Deoarece setul de testare poate avea o distributie a datelor diferita si valori minime si maxime diferite, acest lucru poate duce la valori mai mici sau mai mari decat 0 si 1.\n",
    "\n",
    "2. Daca rulam de mai multe ori codul de mai sus, vectorii `theta_norm` si `theta` au de fiecare data valori diferite. De ce? Considerati efectul functiei  `split_train_test`. \n",
    "Ruland de mai multe ori, functia split_train_test va impartii in functie de un procent random, generat per rulare. Din aceasta cauza, impartirea seturilor de antrenare si testare va fi diferita.\n",
    "\n",
    "3. Cum puteti asigura reproductibilitatea - obtinerea acelorasi rezulatte - la rulari diferite?\n",
    "Putem impartii seturile de antrenare si testare dupa un procent constant, nu unul generat random.\n",
    "\n",
    "4. Obtineti vectorul de valori MSE evaluate la fiecare epoca pe setul de testare si reprezentati pe acelasi grafic valoarea functiei de eroare atat pentru setul de antrenare, cat si pentru setul de testare. In implementarea curenta valoarea functiei de eroare pe setul de testare se determina la fiecare `log_interval` pasi.\n",
    "? uh am i supposed to modify code here ?\n",
    "\n",
    "5. Rulati codul pentru valorile ne-scalate ale matricelor X_train si X_test. Ce observati?\n",
    "Lipsa valorilor ne-scalate duce la ingreunarea invatarii corecte a datelor. (uh maybe rephrase this one it's kinda ambiguous)\n",
    "\n",
    "6. Rulati codul folosind valorile scalate ale matricelor X_train si X_test, dar fara adaugarea unei coloane de 1. Comparati valorile de eroare pe seturile de antrenare si de testare pentru cele doua rulari: cu si fara coloana de 1. \\\n",
    "Daca nu se adauga coloana de 1 algoritmul nu poate invata sa aproximeze corect termenul liber, astfel invatand doar relatii liniare care trec prin originea coordonatelor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dee5c9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
